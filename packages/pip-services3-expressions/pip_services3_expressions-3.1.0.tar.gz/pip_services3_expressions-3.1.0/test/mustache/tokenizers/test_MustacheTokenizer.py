# -*- coding: utf-8 -*-
from pip_services3_expressions.calculator.tokenizers.MustacheTokenizer import MustacheTokenizer
from pip_services3_expressions.tokenizers.Token import Token
from pip_services3_expressions.tokenizers.TokenType import TokenType
from test.tokenizers.TokenizerFixture import TokenizerFixture


class TestMustacheTokenizer:

    def test_template_1(self):
        token_string = "Hello, {{ Name }}!"
        expected_tokens = [
            Token(TokenType.Special, "Hello, "),
            Token(TokenType.Symbol, "{{"),
            Token(TokenType.Whitespace, " "),
            Token(TokenType.Word, "Name"),
            Token(TokenType.Whitespace, " "),
            Token(TokenType.Symbol, "}}"),
            Token(TokenType.Special, "!"),
        ]

        tokenizer = MustacheTokenizer()
        tokenizer.skip_eof = True
        token_list = tokenizer.tokenize_buffer(token_string)

        TokenizerFixture.assert_are_equals_token_lists(expected_tokens, token_list)

    def test_template_2(self):
        token_string = "Hello, {{{ Name }}}!"
        expected_tokens = [
            Token(TokenType.Special, "Hello, "),
            Token(TokenType.Symbol, "{{{"),
            Token(TokenType.Whitespace, " "),
            Token(TokenType.Word, "Name"),
            Token(TokenType.Whitespace, " "),
            Token(TokenType.Symbol, "}}}"),
            Token(TokenType.Special, "!"),
        ]

        tokenizer = MustacheTokenizer()
        tokenizer.skip_eof = True
        token_list = tokenizer.tokenize_buffer(token_string)

        TokenizerFixture.assert_are_equals_token_lists(expected_tokens, token_list)

    def test_template_3(self):
        token_string = "{{ Name }}}"
        expected_tokens = [
            Token(TokenType.Symbol, "{{"),
            Token(TokenType.Whitespace, " "),
            Token(TokenType.Word, "Name"),
            Token(TokenType.Whitespace, " "),
            Token(TokenType.Symbol, "}}}")
        ]

        tokenizer = MustacheTokenizer()
        tokenizer.skip_eof = True
        token_list = tokenizer.tokenize_buffer(token_string)

        TokenizerFixture.assert_are_equals_token_lists(expected_tokens, token_list)

    def test_template_4(self):
        token_string = "Hello, World!"
        expected_tokens = [
            Token(TokenType.Special, "Hello, World!")
        ]

        tokenizer = MustacheTokenizer()
        tokenizer.skip_eof = True
        token_list = tokenizer.tokenize_buffer(token_string)

        TokenizerFixture.assert_are_equals_token_lists(expected_tokens, token_list)
