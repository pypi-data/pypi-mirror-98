Data collection, processing and analysis are rarely one and done tasks. Even if you're
lucky enough that your problem is small enough to be solved in a single script, running
your workflow from end to end might take too long to run every time you make a change.
If you have a more complex problem, where the output of one step becomes the input for
the next step, keeping track of what needs to be run and then running only the parts you
need is tedious. If some parts of your workflow need to be running all of the time, and
others only every now and again this gets harder again.

Untitled Job Runner (ujr from now on) is intended to be a building block for solving
some of the common problems in writing and executing complex workflows. It provides a
framework for breaking down your work into `jobs` that represent interrelated workflows,
and `tasks` that represent the actual work to be executed. Once your workflow is
represented as a job that can generate tasks, Untitled Job Runner takes care of the
necessary steps of executing that work, including:

- Working out what tasks are ready to run (through the job representing the work).
- Running those tasks in parallel to take advantage of your available compute.
- Making sure that the same task only runs once.
- Logging and reporting task success, and catching, reporting and retrying tasks 
  on failure.

For example, you might have a job that describes a realtime data collection through a
Twitter API:

- One continuous task might represent the streaming of tweets to a file that is rolled over on a regular basis.
- A batch task might find Youtube video links being shared in the tweets in each file, then make batch calls to the Youtube API to collect complete metadata for each video. 
- Another task might then download the thumbnails for all of the media shared in each of of the files.
- A final task might periodically archive all of the finalised files generated by the other tasks to a cloud service.

This framework has evolved out of a university research infrastructure facility that
collects data, stores it, and processes it in a variety of ways for a variety of
research projects with differing needs. The initial development has been with
a data infrastructure / data science use case in mind, and we hope it will be useful
for other use cases as well.

Compared to other approaches
____________________________

Unlike other approaches ujr does not intend to be a complete end to end solution. It is
a building block for your own solution. Unlike larger frameworks such as airflow and
luigi, it is designed to scale down to smaller problems (think of running it on your
laptop), rather than requiring deploying a whole environment including servers,
databases and worker nodes to get started. Unlike other options such as make or pydoit,
it easily handles a mixture of continuous, batch and ad-hoc tasks.

Instead of prescribing a specific framework or architecture, ujr aims to add hookpoints 
and lifecycle steps that make it easy to integrate with other systems. Furthermore,
ujr makes no attempt to model workflows as dependency graphs: a job is free to generate
arbitrary tasks, and the runner will do it's best to run them. 

