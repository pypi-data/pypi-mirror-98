{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "qT20LFmb3jSW",
    "outputId": "595c3efa-50dd-4f5a-9695-02ab51af4e61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.__version__: 1.6.0\n",
      "torch device: cpu\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import time\n",
    "from zipfile import ZipFile\n",
    "import unicodedata\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'torch.__version__: {torch.__version__}')\n",
    "print(f'torch device: {device}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NAuXJjo9NuT8"
   },
   "source": [
    "## Dataset: Phrase pairs (2 languages)\n",
    "\n",
    "Download [manythings.org/anki/spa-eng.zip](http://www.manythings.org/anki/spa-eng.zip) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "63Ox1YURzVhF",
    "outputId": "1fe23f41-d672-4ddf-9e4f-cabca9de294b"
   },
   "outputs": [],
   "source": [
    "dataset_path = Path('../data/spa.txt')\n",
    "if not dataset_path.exists():\n",
    "    with ZipFile('../data/spa-eng.zip', 'r') as zipobj:\n",
    "       # Get a list of all archived file names from the zip\n",
    "       # filenames = zipobj.namelist()\n",
    "       zipobj.extract('spa.txt')\n",
    "\n",
    "    # df = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "JD8Qy0eC0ZtA"
   },
   "outputs": [],
   "source": [
    "lines = dataset_path.open(encoding='UTF-8').read().strip().split('\\n')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "JouLb6Eo4f28"
   },
   "outputs": [],
   "source": [
    "num_examples = 100  # toy problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "as2-5vGn4jUa"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>spanish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>No way!</td>\n",
       "      <td>¡De eso nada!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>No way!</td>\n",
       "      <td>¡Ni cagando!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>No way!</td>\n",
       "      <td>¡Mangos!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>No way!</td>\n",
       "      <td>¡Minga!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>No way!</td>\n",
       "      <td>¡Ni en pedo!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    english        spanish\n",
       "95  No way!  ¡De eso nada!\n",
       "96  No way!   ¡Ni cagando!\n",
       "97  No way!       ¡Mangos!\n",
       "98  No way!        ¡Minga!\n",
       "99  No way!   ¡Ni en pedo!"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 100 rows = toy problem\n",
    "df = pd.read_csv(dataset_path, sep='\\t', nrows=100, header=None, usecols=range(2)) \n",
    "df.columns = 'english spanish'.split()\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "jCUSf31E4m6t"
   },
   "outputs": [],
   "source": [
    "def preprocess_sentence(s):\n",
    "    \"\"\" Tokenize with simple multilingual tokenizer plus add <start> and <end> tokens\n",
    "    \n",
    "    Adds space between a word and the punctuation following it, so token\n",
    "    >>> preprocess(\" Hola!   ¿Que tal?   \")\n",
    "    \"Hola ! ¿ Que tal ?\"\n",
    "    \n",
    "    Reference:\n",
    "        https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    \"\"\"\n",
    "    s = re.sub(r'([?.!,¿\"\"\\'])', r' \\1 ', s)\n",
    "    s = re.sub(r'[ ]+', ' ', s)\n",
    "    # replace everything with space except (a-z, A-Z, \"-\", \".\", \"?\", \"!\", \",\")\n",
    "    s = re.sub(r\"[^-a-zA-Z?.!,¿]+\", \" \", s)\n",
    "    s = s.strip()\n",
    "    # adding a start and an end token to the sentence so RNN will work on variable length text\n",
    "    return '<start> ' + s + ' <stop>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>spanish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>&lt;start&gt; I ran . &lt;stop&gt;</td>\n",
       "      <td>&lt;start&gt; Corri . &lt;stop&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>&lt;start&gt; I ran . &lt;stop&gt;</td>\n",
       "      <td>&lt;start&gt; Corri a . &lt;stop&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>&lt;start&gt; Attack ! &lt;stop&gt;</td>\n",
       "      <td>&lt;start&gt; Ataque ! &lt;stop&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>&lt;start&gt; Listen . &lt;stop&gt;</td>\n",
       "      <td>&lt;start&gt; Escuchen . &lt;stop&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>&lt;start&gt; Get up . &lt;stop&gt;</td>\n",
       "      <td>&lt;start&gt; Levanta . &lt;stop&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    english                    spanish\n",
       "35   <start> I ran . <stop>     <start> Corri . <stop>\n",
       "36   <start> I ran . <stop>   <start> Corri a . <stop>\n",
       "50  <start> Attack ! <stop>    <start> Ataque ! <stop>\n",
       "89  <start> Listen . <stop>  <start> Escuchen . <stop>\n",
       "53  <start> Get up . <stop>   <start> Levanta . <stop>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for c in df.columns:\n",
    "    df[c] = df[c].apply(lambda s: unicodedata.normalize('NFD', s))\n",
    "    df[c] = df[c].apply(lambda s: preprocess_sentence(s))\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "2rXA7-N34sok"
   },
   "outputs": [],
   "source": [
    "class LanguageIndex():\n",
    "    \"\"\" Create vocabulary mapping and index (inverse mapping)\n",
    "    \n",
    "    >>> langindex = LanguageIndex(df['english'])\n",
    "    >>> langindex.word2idx.items()[:3]\n",
    "    {\"papa\": 5, ...\n",
    "    >>> langindex.idx2word.items()[:3]\n",
    "    {5: \"papa\"}\n",
    "    \"\"\"\n",
    "    def __init__(self, phrases):\n",
    "        \"\"\" `phrases` is a list of phrases in one language \"\"\"\n",
    "        self.word2idx = {}\n",
    "        self.vocab = []\n",
    "        self.idx2word = self.vocab  # this can just be a list\n",
    "        self.create_index(phrases)\n",
    "        \n",
    "    def create_index(self, phrases):\n",
    "        self.vocab = set('<start> <end> <pad>'.split())\n",
    "        for phrase in phrases:\n",
    "            self.vocab.update(set(phrase.split()))\n",
    "        self.vocab = sorted(self.vocab)\n",
    "\n",
    "        self.idx2word = self.vocab\n",
    "        self.word2idx = dict(zip(self.vocab, range(len(self.vocab))))\n",
    "    \n",
    "    def __getitem__(self, tok):\n",
    "        return self.word2idx.get(tok) or self.vocab[tok]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "Fesymsn34v7z",
    "outputId": "43e7a6c8-8505-4dd5-a2d7-4a82efae0ab3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>75</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>76</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>73</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>72</td>\n",
       "      <td>123</td>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1    2  3    4\n",
       "0  5  75    2  6  NaN\n",
       "1  5  76    2  6  NaN\n",
       "2  5  73    2  6  NaN\n",
       "3  5  72  123  2  6.0\n",
       "4  5  42    2  6  NaN"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# index language using the class above\n",
    "targetlang = \"english\"\n",
    "sourcelang = \"spanish\"\n",
    "inp_index = LanguageIndex(phrases=df[sourcelang].values)\n",
    "targ_index = LanguageIndex(df[targetlang].values)\n",
    "# Vectorize the input and target languages\n",
    "input_tensors = [[inp_index.word2idx[s] for s in es.split(' ')]  for es in df[sourcelang].values.tolist()]\n",
    "target_tensors = [[targ_index.word2idx[s] for s in eng.split(' ')]  for eng in df[targetlang].values.tolist()]\n",
    "pd.DataFrame(input_tensors[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "uPordlA-N4qR",
    "outputId": "b15fd270-d92c-4e85-cc83-ff9fb478780c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1  2  3\n",
       "0  4  10  1  5\n",
       "1  4  10  1  5\n",
       "2  4  10  1  5\n",
       "3  4  10  1  5\n",
       "4  4  15  1  5"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(target_tensors[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "8cwX-0rt4zmN"
   },
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "ycYy5gq641Uy"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 6)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the max_length of input and output tensor\n",
    "max_length_inp = max(len(t) for t in input_tensors)\n",
    "max_length_tar = max(len(t) for t in target_tensors)\n",
    "max_length_inp, max_length_tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "q05E5IwH42_1"
   },
   "outputs": [],
   "source": [
    "def pad_seq(s, max_len, pad_tok_idx):\n",
    "    padded = pad_tok_idx * np.ones(max_len, dtype=np.int64)  # FIXME: int16 should be pleanty\n",
    "    s_len = min(max_len, len(s))\n",
    "    padded[:s_len] = s[:s_len]\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "66dJPqzV44jd",
    "outputId": "8e5fe7eb-f39d-4365-a9fb-63fdaeff8f34"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inplace padding\n",
    "input_tensors = [pad_seq(x, max_length_inp, inp_index['<pad>']) for x in input_tensors]\n",
    "target_tensors = [pad_seq(x, max_length_tar, targ_index['<pad>']) for x in target_tensors]\n",
    "len(target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "zvatfCWS46T-",
    "outputId": "2eeff7ec-2c61-4e8e-d084-ed14251a3bca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100, 100, 100)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating training and validation sets using an 80-20 split\n",
    "# input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "input_tensor_train = input_tensor_val = input_tensors\n",
    "target_tensor_train = target_tensor_val = target_tensors\n",
    "# Show length\n",
    "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "-QRQKwxf479Q"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "IDSxA4OM5Qlp"
   },
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    \"\"\" Convert each vector to torch.tensor type and wrap with Dataloader() \"\"\"\n",
    "    def __init__(self, X, y):\n",
    "        self.data = X\n",
    "        self.target = y\n",
    "        # FIXME: vectorize with torch.tensor\n",
    "        self.length = [ np.sum(1 - np.equal(x, 0)) for x in X]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.target[index]\n",
    "        x_len = self.length[index]\n",
    "        return x,y,x_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "s3Be7lOZ5R-d"
   },
   "outputs": [],
   "source": [
    "train_dataset = TranslationDataset(input_tensor_train, target_tensor_train)\n",
    "\n",
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 16\n",
    "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
    "embedding_dim = 32\n",
    "units = 32\n",
    "vocab_inp_size = len(inp_index.word2idx)\n",
    "vocab_tar_size = len(targ_index.word2idx)\n",
    "\n",
    "dataset = DataLoader(train_dataset,\n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     drop_last=True,\n",
    "                     shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "blYXo7pv5TOu"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super().__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        self.gru = nn.GRU(self.embedding_dim, self.enc_units)\n",
    "        \n",
    "    def forward(self, x, lens, device=device):\n",
    "        # x: batch_size, max_length \n",
    "        \n",
    "        # x: batch_size, max_length, embedding_dim\n",
    "        x = self.embedding(x) \n",
    "                \n",
    "        # x transformed = max_len X batch_size X embedding_dim\n",
    "        # x = x.permute(1,0,2)\n",
    "        x = pack_padded_sequence(x, lens) # unpad\n",
    "    \n",
    "        self.hidden = self.initialize_hidden_state(device)\n",
    "        \n",
    "        # output: max_length, batch_size, enc_units\n",
    "        # self.hidden: 1, batch_size, enc_units\n",
    "        output, self.hidden = self.gru(x, self.hidden) # gru returns hidden state of all timesteps as well as hidden state at last timestep\n",
    "        \n",
    "        # pad the sequence to the max length in the batch\n",
    "        output, _ = pad_packed_sequence(output)\n",
    "        \n",
    "        return output, self.hidden\n",
    "\n",
    "    def initialize_hidden_state(self, device=device):\n",
    "        return torch.zeros((1, self.batch_sz, self.enc_units)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "SrsQ7dTg5V__"
   },
   "outputs": [],
   "source": [
    "def sort_batch(X, y, lengths):\n",
    "    \"\"\" Sort batch function to be able to use with pad_packed_sequence \"\"\"\n",
    "    lengths, indx = lengths.sort(dim=0, descending=True)\n",
    "    X = X[indx]\n",
    "    y = y[indx]\n",
    "    return X.transpose(0,1), y, lengths # transpose (batch x seq) to (seq x batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set pairs (source then target):...\n",
      "\n",
      "<start> Go . <stop> <pad> <pad>\n",
      "<start> Ve . <stop> <pad> <pad> <pad>\n",
      "\n",
      "<start> Go . <stop> <pad> <pad>\n",
      "<start> Vete . <stop> <pad> <pad> <pad>\n",
      "\n",
      "<start> Go . <stop> <pad> <pad>\n",
      "<start> Vaya . <stop> <pad> <pad> <pad>\n",
      "\n",
      "<start> Go . <stop> <pad> <pad>\n",
      "<start> Va yase . <stop> <pad> <pad>\n",
      "\n",
      "<start> Hi . <stop> <pad> <pad>\n",
      "<start> Hola . <stop> <pad> <pad> <pad>\n",
      "\n",
      "<start> Run ! <stop> <pad> <pad>\n",
      "<start> Corre ! <stop> <pad> <pad> <pad>\n",
      "\n",
      "<start> Run ! <stop> <pad> <pad>\n",
      "<start> Corran ! <stop> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "print('Training set pairs (source then target):...')\n",
    "for rownum, (inp, targ) in enumerate(zip(input_tensors, target_tensors)):\n",
    "    print()\n",
    "    print(' '.join([targ_index.idx2word[i] for i in targ]))\n",
    "    print(' '.join([inp_index.idx2word[i] for i in inp]))\n",
    "    if rownum > 5:\n",
    "        break   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2X1h155CPQ1Y"
   },
   "source": [
    "## Testing the Encoder\n",
    "Before proceeding with training, we should always try to test out model behavior such as the size of outputs just to make that things are going as expected. In PyTorch this can be done easily since everything comes in eager execution by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "rbSLACY45Xz-",
    "outputId": "d6461fcf-eba8-4843-f2d2-e5d77e788696"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output tensor should be size (max_length, batch_size, num_enc_units):\n",
      "torch.Size([7, 16, 32])\n",
      "torch.Size([16, 7])\n",
      "torch.Size([16, 6])\n",
      "inp_batch_sorted.size() seems wrong: torch.Size([7, 16])\n",
      "out_batch_sorted.size() seems correct: torch.Size([16, 6])\n",
      "BATCH_SIZE: 16\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "encoder.to(device)\n",
    "\n",
    "# obtain one sample from the data iterator\n",
    "it = iter(dataset)\n",
    "inp_batch, out_batch, inp_batch_len = next(it)\n",
    "\n",
    "# sort the batch first to be able to use with pac_pack_sequence\n",
    "inp_batch_sorted, out_batch_sorted, lengths = sort_batch(inp_batch, out_batch, inp_batch_len)\n",
    "\n",
    "enc_output, enc_hidden = encoder(inp_batch_sorted.to(device), lengths, device=device)\n",
    "\n",
    "print('Encoder output tensor should be size (max_length, batch_size, num_enc_units):')\n",
    "print(enc_output.size()) \n",
    "print(inp_batch.size())\n",
    "print(out_batch.size())\n",
    "print(f'inp_batch_sorted.size() seems wrong: {inp_batch_sorted.size()}')\n",
    "print(f'out_batch_sorted.size() seems correct: {out_batch_sorted.size()}')\n",
    "print(f'BATCH_SIZE: {BATCH_SIZE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QiMRxHQFGPtt"
   },
   "source": [
    "### Decoder\n",
    "\n",
    "Here, we'll implement an encoder-decoder model with attention which you can read about in the TensorFlow [Neural Machine Translation (seq2seq) tutorial](https://github.com/tensorflow/nmt). This example uses a more recent set of APIs. This notebook implements the [attention equations](https://github.com/tensorflow/nmt#background-on-the-attention-mechanism) from the seq2seq tutorial. The following diagram shows that each input word is assigned a weight by the attention mechanism which is then used by the decoder to predict the next word in the sentence.\n",
    "\n",
    "<img src=\"https://www.tensorflow.org/images/seq2seq/attention_mechanism.jpg\" width=\"500\" alt=\"attention mechanism\">\n",
    "\n",
    "The input is put through an encoder model which gives us the encoder output of shape *(batch_size, max_length, hidden_size)* and the encoder hidden state of shape *(batch_size, hidden_size)*. \n",
    "\n",
    "Here are the equations that are implemented:\n",
    "\n",
    "<img src=\"https://www.tensorflow.org/images/seq2seq/attention_equation_0.jpg\" alt=\"attention equation 0\" width=\"800\">\n",
    "<img src=\"https://www.tensorflow.org/images/seq2seq/attention_equation_1.jpg\" alt=\"attention equation 1\" width=\"800\">\n",
    "\n",
    "We're using *Bahdanau attention*. Lets decide on notation before writing the simplified form:\n",
    "\n",
    "* FC = Fully connected (dense) layer\n",
    "* EO = Encoder output\n",
    "* H = hidden state\n",
    "* X = input to the decoder\n",
    "\n",
    "And the pseudo-code:\n",
    "\n",
    "* `score = FC(tanh(FC(EO) + FC(H)))`\n",
    "* `attention weights = softmax(score, axis = 1)`. Softmax by default is applied on the last axis but here we want to apply it on the *1st axis*, since the shape of score is *(batch_size, max_length, 1)*. `Max_length` is the length of our input. Since we are trying to assign a weight to each input, softmax should be applied on that axis.\n",
    "* `context vector = sum(attention weights * EO, axis = 1)`. Same reason as above for choosing axis as 1.\n",
    "* `embedding output` = The input to the decoder X is passed through an embedding layer.\n",
    "* `merged vector = concat(embedding output, context vector)`\n",
    "* This merged vector is then given to the GRU\n",
    "  \n",
    "The shapes of all the vectors at each step have been specified in the comments in the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "t4djvgil5bMQ"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, enc_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.enc_units = enc_units\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        self.gru = nn.GRU(self.embedding_dim + self.enc_units, \n",
    "                          self.dec_units,\n",
    "                          batch_first=True)\n",
    "        self.fc = nn.Linear(self.enc_units, self.vocab_size)\n",
    "        \n",
    "        # used for attention\n",
    "        self.W1 = nn.Linear(self.enc_units, self.dec_units)\n",
    "        self.W2 = nn.Linear(self.enc_units, self.dec_units)\n",
    "        self.V = nn.Linear(self.enc_units, 1)\n",
    "    \n",
    "    def forward(self, x, hidden, enc_output):\n",
    "        # enc_output original: (max_length, batch_size, enc_units)\n",
    "        # enc_output converted == (batch_size, max_length, hidden_size)\n",
    "        enc_output = enc_output.permute(1,0,2)\n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        \n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        hidden_with_time_axis = hidden.permute(1, 0, 2)\n",
    "        \n",
    "        # score: (batch_size, max_length, hidden_size) # Bahdanaus's\n",
    "        # we get 1 at the last axis because we are applying tanh(FC(EO) + FC(H)) to self.V\n",
    "        # It doesn't matter which FC we pick for each of the inputs\n",
    "        score = torch.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis))\n",
    "        \n",
    "        #score = torch.tanh(self.W2(hidden_with_time_axis) + self.W1(enc_output))\n",
    "          \n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        attention_weights = torch.softmax(self.V(score), dim=1)\n",
    "        \n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * enc_output\n",
    "        context_vector = torch.sum(context_vector, dim=1)\n",
    "        \n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        # takes case of the right portion of the model above (illustrated in red)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        #x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        # ? Looks like attention vector in diagram of source\n",
    "        x = torch.cat((context_vector.unsqueeze(1), x), -1)\n",
    "        \n",
    "        # passing the concatenated vector to the GRU\n",
    "        # output: (batch_size, 1, hidden_size)\n",
    "        output, state = self.gru(x)\n",
    "        \n",
    "        \n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output =  output.view(-1, output.size(2))\n",
    "        \n",
    "        # output shape == (batch_size * 1, vocab)\n",
    "        x = self.fc(output)\n",
    "        \n",
    "        return x, state, attention_weights\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return torch.zeros((1, self.batch_sz, self.dec_units))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HsG5We7Sk_UR"
   },
   "source": [
    "## Testing the Decoder\n",
    "Similarily, try to test the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "id": "lmipPRVx5fqO",
    "outputId": "d9c4cbf1-02b9-4be1-8f6f-340a42bd38ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  torch.Size([16, 7])\n",
      "Output:  torch.Size([16, 6])\n",
      "Encoder Output:  torch.Size([7, 16, 32])\n",
      "Encoder Hidden:  torch.Size([1, 16, 32])\n",
      "Decoder Input:  torch.Size([16, 1])\n",
      "--------\n",
      "Prediction:  torch.Size([16, 56])\n",
      "Decoder Hidden:  torch.Size([1, 16, 32])\n",
      "torch.Size([16, 1])\n"
     ]
    }
   ],
   "source": [
    "# obtain one sample from the data iterator\n",
    "it = iter(dataset)\n",
    "x, y, x_len = next(it)\n",
    "\n",
    "print(\"Input: \", x.shape)\n",
    "print(\"Output: \", y.shape)\n",
    "\n",
    "# sort the batch first to be able to use with pac_pack_sequence\n",
    "xs, ys, lens = sort_batch(x, y, x_len)\n",
    "\n",
    "enc_output, enc_hidden = encoder(xs.to(device), lens, device=device)\n",
    "print(\"Encoder Output: \", enc_output.shape) # batch_size X max_length X enc_units\n",
    "print(\"Encoder Hidden: \", enc_hidden.shape) # batch_size X enc_units (corresponds to the last state)\n",
    "\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, units, BATCH_SIZE)\n",
    "decoder = decoder.to(device)\n",
    "\n",
    "#print(enc_hidden.squeeze(0).shape)\n",
    "\n",
    "dec_hidden = enc_hidden#.squeeze(0)\n",
    "dec_input = torch.tensor([[targ_index.word2idx['<start>']]] * BATCH_SIZE)\n",
    "print(\"Decoder Input: \", dec_input.shape)\n",
    "print(\"--------\")\n",
    "\n",
    "for t in range(1, y.size(1)):\n",
    "    # enc_hidden: 1, batch_size, enc_units\n",
    "    # output: max_length, batch_size, enc_units\n",
    "    predictions, dec_hidden, _ = decoder(dec_input.to(device), \n",
    "                                         dec_hidden.to(device), \n",
    "                                         enc_output.to(device))\n",
    "    \n",
    "    print(\"Prediction: \", predictions.shape)\n",
    "    print(\"Decoder Hidden: \", dec_hidden.shape)\n",
    "    \n",
    "    #loss += loss_function(y[:, t].to(device), predictions.to(device))\n",
    "    \n",
    "    dec_input = y[:, t].unsqueeze(1)\n",
    "    print(dec_input.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "QclyWIop5dRG"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    \"\"\" Only consider non-zero inputs in the loss; mask needed \"\"\"\n",
    "    #mask = 1 - np.equal(real, 0) # assign 0 to all above 0 and 1 to all 0s\n",
    "    #print(mask)\n",
    "    mask = real.ge(1).type(torch.cuda.FloatTensor if device is 'gpu' else torch.FloatTensor)\n",
    "    \n",
    "    loss_ = criterion(pred, real) * mask \n",
    "    return torch.mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "LjMMYJv85hVT"
   },
   "outputs": [],
   "source": [
    "## TODO: Combine the encoder and decoder into one class\n",
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, units, BATCH_SIZE)\n",
    "\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), \n",
    "                       lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6_WoDZM7reU"
   },
   "source": [
    "## Training\n",
    "Now we start the training. We are only using 10 epochs but you can expand this to keep trainining the model for a longer period of time. Note that in this case we are teacher forcing during training. Find a more detailed explanation in the official TensorFlow [implementation](https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/nmt_with_attention/nmt_with_attention.ipynb) of this notebook provided by the TensorFlow team. \n",
    "\n",
    "- Pass the input through the encoder which return encoder output and the encoder hidden state.\n",
    "- The encoder output, encoder hidden state and the decoder input (which is the start token) is passed to the decoder.\n",
    "- The decoder returns the predictions and the decoder hidden state.\n",
    "- The decoder hidden state is then passed back into the model and the predictions are used to calculate the loss.\n",
    "- Use teacher forcing to decide the next input to the decoder.\n",
    "- Teacher forcing is the technique where the target word is passed as the next input to the decoder.\n",
    "- The final step is to calculate the gradients and apply it to the optimizer and backpropagate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1207
    },
    "id": "KN8G-3YY8ADm",
    "outputId": "52c3acf4-10be-48c5-b305-4c6d24627517"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 0.3814\n",
      "Epoch 1 Loss 0.3371\n",
      "Time taken for 1 epoch 0.08265995979309082 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.3344\n",
      "Epoch 2 Loss 0.3316\n",
      "Time taken for 1 epoch 0.06778597831726074 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.4564\n",
      "Epoch 3 Loss 0.3316\n",
      "Time taken for 1 epoch 0.06458878517150879 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.2913\n",
      "Epoch 4 Loss 0.3303\n",
      "Time taken for 1 epoch 0.06866931915283203 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.3079\n",
      "Epoch 5 Loss 0.3202\n",
      "Time taken for 1 epoch 0.06520199775695801 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.3100\n",
      "Epoch 6 Loss 0.3212\n",
      "Time taken for 1 epoch 0.0650491714477539 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.3310\n",
      "Epoch 7 Loss 0.3212\n",
      "Time taken for 1 epoch 0.06708288192749023 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.2792\n",
      "Epoch 8 Loss 0.3120\n",
      "Time taken for 1 epoch 0.06485843658447266 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.2983\n",
      "Epoch 9 Loss 0.3054\n",
      "Time taken for 1 epoch 0.06516551971435547 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.3141\n",
      "Epoch 10 Loss 0.3094\n",
      "Time taken for 1 epoch 0.0668485164642334 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 0.2720\n",
      "Epoch 11 Loss 0.3020\n",
      "Time taken for 1 epoch 0.07110333442687988 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 0.3059\n",
      "Epoch 12 Loss 0.3040\n",
      "Time taken for 1 epoch 0.0655374526977539 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 0.3863\n",
      "Epoch 13 Loss 0.2947\n",
      "Time taken for 1 epoch 0.06612777709960938 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 0.2131\n",
      "Epoch 14 Loss 0.2957\n",
      "Time taken for 1 epoch 0.06910824775695801 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 0.3343\n",
      "Epoch 15 Loss 0.2925\n",
      "Time taken for 1 epoch 0.06991028785705566 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 0.2656\n",
      "Epoch 16 Loss 0.2844\n",
      "Time taken for 1 epoch 0.06876206398010254 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 0.2786\n",
      "Epoch 17 Loss 0.2848\n",
      "Time taken for 1 epoch 0.07872843742370605 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 0.3018\n",
      "Epoch 18 Loss 0.2758\n",
      "Time taken for 1 epoch 0.07146239280700684 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 0.2762\n",
      "Epoch 19 Loss 0.2745\n",
      "Time taken for 1 epoch 0.0748133659362793 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 0.2841\n",
      "Epoch 20 Loss 0.2753\n",
      "Time taken for 1 epoch 0.06906819343566895 sec\n",
      "\n",
      "Epoch 21 Batch 0 Loss 0.2682\n",
      "Epoch 21 Loss 0.2699\n",
      "Time taken for 1 epoch 0.07209348678588867 sec\n",
      "\n",
      "Epoch 22 Batch 0 Loss 0.2814\n",
      "Epoch 22 Loss 0.2625\n",
      "Time taken for 1 epoch 0.06954002380371094 sec\n",
      "\n",
      "Epoch 23 Batch 0 Loss 0.2791\n",
      "Epoch 23 Loss 0.2642\n",
      "Time taken for 1 epoch 0.06542730331420898 sec\n",
      "\n",
      "Epoch 24 Batch 0 Loss 0.2230\n",
      "Epoch 24 Loss 0.2522\n",
      "Time taken for 1 epoch 0.06781315803527832 sec\n",
      "\n",
      "Epoch 25 Batch 0 Loss 0.2648\n",
      "Epoch 25 Loss 0.2546\n",
      "Time taken for 1 epoch 0.06817269325256348 sec\n",
      "\n",
      "Epoch 26 Batch 0 Loss 0.2972\n",
      "Epoch 26 Loss 0.2491\n",
      "Time taken for 1 epoch 0.07109832763671875 sec\n",
      "\n",
      "Epoch 27 Batch 0 Loss 0.3035\n",
      "Epoch 27 Loss 0.2407\n",
      "Time taken for 1 epoch 0.08495545387268066 sec\n",
      "\n",
      "Epoch 28 Batch 0 Loss 0.2532\n",
      "Epoch 28 Loss 0.2439\n",
      "Time taken for 1 epoch 0.07306575775146484 sec\n",
      "\n",
      "Epoch 29 Batch 0 Loss 0.2595\n",
      "Epoch 29 Loss 0.2418\n",
      "Time taken for 1 epoch 0.07785749435424805 sec\n",
      "\n",
      "Epoch 30 Batch 0 Loss 0.2730\n",
      "Epoch 30 Loss 0.2401\n",
      "Time taken for 1 epoch 0.07415175437927246 sec\n",
      "\n",
      "Epoch 31 Batch 0 Loss 0.2590\n",
      "Epoch 31 Loss 0.2358\n",
      "Time taken for 1 epoch 0.06800246238708496 sec\n",
      "\n",
      "Epoch 32 Batch 0 Loss 0.2277\n",
      "Epoch 32 Loss 0.2333\n",
      "Time taken for 1 epoch 0.06496524810791016 sec\n",
      "\n",
      "Epoch 33 Batch 0 Loss 0.2385\n",
      "Epoch 33 Loss 0.2277\n",
      "Time taken for 1 epoch 0.06822323799133301 sec\n",
      "\n",
      "Epoch 34 Batch 0 Loss 0.1937\n",
      "Epoch 34 Loss 0.2248\n",
      "Time taken for 1 epoch 0.06891822814941406 sec\n",
      "\n",
      "Epoch 35 Batch 0 Loss 0.2276\n",
      "Epoch 35 Loss 0.2183\n",
      "Time taken for 1 epoch 0.07457566261291504 sec\n",
      "\n",
      "Epoch 36 Batch 0 Loss 0.2717\n",
      "Epoch 36 Loss 0.2205\n",
      "Time taken for 1 epoch 0.07567405700683594 sec\n",
      "\n",
      "Epoch 37 Batch 0 Loss 0.2003\n",
      "Epoch 37 Loss 0.2148\n",
      "Time taken for 1 epoch 0.07049870491027832 sec\n",
      "\n",
      "Epoch 38 Batch 0 Loss 0.1980\n",
      "Epoch 38 Loss 0.2146\n",
      "Time taken for 1 epoch 0.0693655014038086 sec\n",
      "\n",
      "Epoch 39 Batch 0 Loss 0.1931\n",
      "Epoch 39 Loss 0.2022\n",
      "Time taken for 1 epoch 0.06989455223083496 sec\n",
      "\n",
      "Epoch 40 Batch 0 Loss 0.1235\n",
      "Epoch 40 Loss 0.2024\n",
      "Time taken for 1 epoch 0.06908559799194336 sec\n",
      "\n",
      "Epoch 41 Batch 0 Loss 0.2057\n",
      "Epoch 41 Loss 0.1967\n",
      "Time taken for 1 epoch 0.06880784034729004 sec\n",
      "\n",
      "Epoch 42 Batch 0 Loss 0.2329\n",
      "Epoch 42 Loss 0.1977\n",
      "Time taken for 1 epoch 0.0701284408569336 sec\n",
      "\n",
      "Epoch 43 Batch 0 Loss 0.2190\n",
      "Epoch 43 Loss 0.1960\n",
      "Time taken for 1 epoch 0.06910419464111328 sec\n",
      "\n",
      "Epoch 44 Batch 0 Loss 0.2451\n",
      "Epoch 44 Loss 0.1916\n",
      "Time taken for 1 epoch 0.0697488784790039 sec\n",
      "\n",
      "Epoch 45 Batch 0 Loss 0.1249\n",
      "Epoch 45 Loss 0.1952\n",
      "Time taken for 1 epoch 0.06972932815551758 sec\n",
      "\n",
      "Epoch 46 Batch 0 Loss 0.2142\n",
      "Epoch 46 Loss 0.1888\n",
      "Time taken for 1 epoch 0.06962251663208008 sec\n",
      "\n",
      "Epoch 47 Batch 0 Loss 0.1800\n",
      "Epoch 47 Loss 0.1828\n",
      "Time taken for 1 epoch 0.0708317756652832 sec\n",
      "\n",
      "Epoch 48 Batch 0 Loss 0.2274\n",
      "Epoch 48 Loss 0.1826\n",
      "Time taken for 1 epoch 0.07383275032043457 sec\n",
      "\n",
      "Epoch 49 Batch 0 Loss 0.1963\n",
      "Epoch 49 Loss 0.1782\n",
      "Time taken for 1 epoch 0.08094143867492676 sec\n",
      "\n",
      "Epoch 50 Batch 0 Loss 0.1338\n",
      "Epoch 50 Loss 0.1782\n",
      "Time taken for 1 epoch 0.08166074752807617 sec\n",
      "\n",
      "Epoch 51 Batch 0 Loss 0.2136\n",
      "Epoch 51 Loss 0.1757\n",
      "Time taken for 1 epoch 0.08011412620544434 sec\n",
      "\n",
      "Epoch 52 Batch 0 Loss 0.1401\n",
      "Epoch 52 Loss 0.1737\n",
      "Time taken for 1 epoch 0.07862162590026855 sec\n",
      "\n",
      "Epoch 53 Batch 0 Loss 0.1825\n",
      "Epoch 53 Loss 0.1723\n",
      "Time taken for 1 epoch 0.07271265983581543 sec\n",
      "\n",
      "Epoch 54 Batch 0 Loss 0.2473\n",
      "Epoch 54 Loss 0.1697\n",
      "Time taken for 1 epoch 0.07149744033813477 sec\n",
      "\n",
      "Epoch 55 Batch 0 Loss 0.1903\n",
      "Epoch 55 Loss 0.1640\n",
      "Time taken for 1 epoch 0.07182168960571289 sec\n",
      "\n",
      "Epoch 56 Batch 0 Loss 0.1727\n",
      "Epoch 56 Loss 0.1632\n",
      "Time taken for 1 epoch 0.07287812232971191 sec\n",
      "\n",
      "Epoch 57 Batch 0 Loss 0.1593\n",
      "Epoch 57 Loss 0.1633\n",
      "Time taken for 1 epoch 0.07938385009765625 sec\n",
      "\n",
      "Epoch 58 Batch 0 Loss 0.1885\n",
      "Epoch 58 Loss 0.1602\n",
      "Time taken for 1 epoch 0.07955241203308105 sec\n",
      "\n",
      "Epoch 59 Batch 0 Loss 0.1974\n",
      "Epoch 59 Loss 0.1527\n",
      "Time taken for 1 epoch 0.07309937477111816 sec\n",
      "\n",
      "Epoch 60 Batch 0 Loss 0.1684\n",
      "Epoch 60 Loss 0.1502\n",
      "Time taken for 1 epoch 0.06671333312988281 sec\n",
      "\n",
      "Epoch 61 Batch 0 Loss 0.1580\n",
      "Epoch 61 Loss 0.1527\n",
      "Time taken for 1 epoch 0.06547975540161133 sec\n",
      "\n",
      "Epoch 62 Batch 0 Loss 0.1811\n",
      "Epoch 62 Loss 0.1464\n",
      "Time taken for 1 epoch 0.06655478477478027 sec\n",
      "\n",
      "Epoch 63 Batch 0 Loss 0.1415\n",
      "Epoch 63 Loss 0.1453\n",
      "Time taken for 1 epoch 0.07374405860900879 sec\n",
      "\n",
      "Epoch 64 Batch 0 Loss 0.1528\n",
      "Epoch 64 Loss 0.1414\n",
      "Time taken for 1 epoch 0.07115364074707031 sec\n",
      "\n",
      "Epoch 65 Batch 0 Loss 0.1607\n",
      "Epoch 65 Loss 0.1420\n",
      "Time taken for 1 epoch 0.0737605094909668 sec\n",
      "\n",
      "Epoch 66 Batch 0 Loss 0.1300\n",
      "Epoch 66 Loss 0.1430\n",
      "Time taken for 1 epoch 0.07287359237670898 sec\n",
      "\n",
      "Epoch 67 Batch 0 Loss 0.1819\n",
      "Epoch 67 Loss 0.1434\n",
      "Time taken for 1 epoch 0.07081961631774902 sec\n",
      "\n",
      "Epoch 68 Batch 0 Loss 0.1392\n",
      "Epoch 68 Loss 0.1359\n",
      "Time taken for 1 epoch 0.06816363334655762 sec\n",
      "\n",
      "Epoch 69 Batch 0 Loss 0.1523\n",
      "Epoch 69 Loss 0.1390\n",
      "Time taken for 1 epoch 0.0688180923461914 sec\n",
      "\n",
      "Epoch 70 Batch 0 Loss 0.1364\n",
      "Epoch 70 Loss 0.1282\n",
      "Time taken for 1 epoch 0.06821489334106445 sec\n",
      "\n",
      "Epoch 71 Batch 0 Loss 0.1646\n",
      "Epoch 71 Loss 0.1301\n",
      "Time taken for 1 epoch 0.06795263290405273 sec\n",
      "\n",
      "Epoch 72 Batch 0 Loss 0.1344\n",
      "Epoch 72 Loss 0.1352\n",
      "Time taken for 1 epoch 0.07096028327941895 sec\n",
      "\n",
      "Epoch 73 Batch 0 Loss 0.1000\n",
      "Epoch 73 Loss 0.1306\n",
      "Time taken for 1 epoch 0.06786632537841797 sec\n",
      "\n",
      "Epoch 74 Batch 0 Loss 0.1290\n",
      "Epoch 74 Loss 0.1241\n",
      "Time taken for 1 epoch 0.06831145286560059 sec\n",
      "\n",
      "Epoch 75 Batch 0 Loss 0.1118\n",
      "Epoch 75 Loss 0.1207\n",
      "Time taken for 1 epoch 0.0690145492553711 sec\n",
      "\n",
      "Epoch 76 Batch 0 Loss 0.1236\n",
      "Epoch 76 Loss 0.1207\n",
      "Time taken for 1 epoch 0.0677194595336914 sec\n",
      "\n",
      "Epoch 77 Batch 0 Loss 0.1117\n",
      "Epoch 77 Loss 0.1252\n",
      "Time taken for 1 epoch 0.0679771900177002 sec\n",
      "\n",
      "Epoch 78 Batch 0 Loss 0.1494\n",
      "Epoch 78 Loss 0.1189\n",
      "Time taken for 1 epoch 0.07319235801696777 sec\n",
      "\n",
      "Epoch 79 Batch 0 Loss 0.1770\n",
      "Epoch 79 Loss 0.1190\n",
      "Time taken for 1 epoch 0.0726480484008789 sec\n",
      "\n",
      "Epoch 80 Batch 0 Loss 0.1025\n",
      "Epoch 80 Loss 0.1164\n",
      "Time taken for 1 epoch 0.06451988220214844 sec\n",
      "\n",
      "Epoch 81 Batch 0 Loss 0.1576\n",
      "Epoch 81 Loss 0.1162\n",
      "Time taken for 1 epoch 0.06515288352966309 sec\n",
      "\n",
      "Epoch 82 Batch 0 Loss 0.1070\n",
      "Epoch 82 Loss 0.1139\n",
      "Time taken for 1 epoch 0.06497645378112793 sec\n",
      "\n",
      "Epoch 83 Batch 0 Loss 0.1445\n",
      "Epoch 83 Loss 0.1116\n",
      "Time taken for 1 epoch 0.06377696990966797 sec\n",
      "\n",
      "Epoch 84 Batch 0 Loss 0.1233\n",
      "Epoch 84 Loss 0.1094\n",
      "Time taken for 1 epoch 0.06721758842468262 sec\n",
      "\n",
      "Epoch 85 Batch 0 Loss 0.1174\n",
      "Epoch 85 Loss 0.1099\n",
      "Time taken for 1 epoch 0.0637667179107666 sec\n",
      "\n",
      "Epoch 86 Batch 0 Loss 0.0912\n",
      "Epoch 86 Loss 0.1082\n",
      "Time taken for 1 epoch 0.06348872184753418 sec\n",
      "\n",
      "Epoch 87 Batch 0 Loss 0.1172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87 Loss 0.1053\n",
      "Time taken for 1 epoch 0.0688626766204834 sec\n",
      "\n",
      "Epoch 88 Batch 0 Loss 0.1078\n",
      "Epoch 88 Loss 0.1075\n",
      "Time taken for 1 epoch 0.06318998336791992 sec\n",
      "\n",
      "Epoch 89 Batch 0 Loss 0.1333\n",
      "Epoch 89 Loss 0.1007\n",
      "Time taken for 1 epoch 0.06417131423950195 sec\n",
      "\n",
      "Epoch 90 Batch 0 Loss 0.1211\n",
      "Epoch 90 Loss 0.1005\n",
      "Time taken for 1 epoch 0.06448531150817871 sec\n",
      "\n",
      "Epoch 91 Batch 0 Loss 0.0783\n",
      "Epoch 91 Loss 0.1030\n",
      "Time taken for 1 epoch 0.06408834457397461 sec\n",
      "\n",
      "Epoch 92 Batch 0 Loss 0.1020\n",
      "Epoch 92 Loss 0.1014\n",
      "Time taken for 1 epoch 0.06280398368835449 sec\n",
      "\n",
      "Epoch 93 Batch 0 Loss 0.0855\n",
      "Epoch 93 Loss 0.0998\n",
      "Time taken for 1 epoch 0.06311249732971191 sec\n",
      "\n",
      "Epoch 94 Batch 0 Loss 0.0702\n",
      "Epoch 94 Loss 0.0960\n",
      "Time taken for 1 epoch 0.06409311294555664 sec\n",
      "\n",
      "Epoch 95 Batch 0 Loss 0.1335\n",
      "Epoch 95 Loss 0.0964\n",
      "Time taken for 1 epoch 0.06370830535888672 sec\n",
      "\n",
      "Epoch 96 Batch 0 Loss 0.0896\n",
      "Epoch 96 Loss 0.0962\n",
      "Time taken for 1 epoch 0.06327104568481445 sec\n",
      "\n",
      "Epoch 97 Batch 0 Loss 0.1609\n",
      "Epoch 97 Loss 0.0938\n",
      "Time taken for 1 epoch 0.06381607055664062 sec\n",
      "\n",
      "Epoch 98 Batch 0 Loss 0.0949\n",
      "Epoch 98 Loss 0.0891\n",
      "Time taken for 1 epoch 0.06476545333862305 sec\n",
      "\n",
      "Epoch 99 Batch 0 Loss 0.0888\n",
      "Epoch 99 Loss 0.0905\n",
      "Time taken for 1 epoch 0.06319117546081543 sec\n",
      "\n",
      "Epoch 100 Batch 0 Loss 0.0675\n",
      "Epoch 100 Loss 0.0918\n",
      "Time taken for 1 epoch 0.06334686279296875 sec\n",
      "\n",
      "Epoch 101 Batch 0 Loss 0.0764\n",
      "Epoch 101 Loss 0.0851\n",
      "Time taken for 1 epoch 0.06463098526000977 sec\n",
      "\n",
      "Epoch 102 Batch 0 Loss 0.0958\n",
      "Epoch 102 Loss 0.0874\n",
      "Time taken for 1 epoch 0.06334328651428223 sec\n",
      "\n",
      "Epoch 103 Batch 0 Loss 0.1125\n",
      "Epoch 103 Loss 0.0887\n",
      "Time taken for 1 epoch 0.06334424018859863 sec\n",
      "\n",
      "Epoch 104 Batch 0 Loss 0.0983\n",
      "Epoch 104 Loss 0.0874\n",
      "Time taken for 1 epoch 0.06308412551879883 sec\n",
      "\n",
      "Epoch 105 Batch 0 Loss 0.1060\n",
      "Epoch 105 Loss 0.0856\n",
      "Time taken for 1 epoch 0.06550359725952148 sec\n",
      "\n",
      "Epoch 106 Batch 0 Loss 0.0617\n",
      "Epoch 106 Loss 0.0848\n",
      "Time taken for 1 epoch 0.06338047981262207 sec\n",
      "\n",
      "Epoch 107 Batch 0 Loss 0.1043\n",
      "Epoch 107 Loss 0.0850\n",
      "Time taken for 1 epoch 0.06322669982910156 sec\n",
      "\n",
      "Epoch 108 Batch 0 Loss 0.1077\n",
      "Epoch 108 Loss 0.0797\n",
      "Time taken for 1 epoch 0.06327581405639648 sec\n",
      "\n",
      "Epoch 109 Batch 0 Loss 0.0817\n",
      "Epoch 109 Loss 0.0800\n",
      "Time taken for 1 epoch 0.06666135787963867 sec\n",
      "\n",
      "Epoch 110 Batch 0 Loss 0.0904\n",
      "Epoch 110 Loss 0.0797\n",
      "Time taken for 1 epoch 0.06344270706176758 sec\n",
      "\n",
      "Epoch 111 Batch 0 Loss 0.0859\n",
      "Epoch 111 Loss 0.0784\n",
      "Time taken for 1 epoch 0.06412220001220703 sec\n",
      "\n",
      "Epoch 112 Batch 0 Loss 0.0724\n",
      "Epoch 112 Loss 0.0796\n",
      "Time taken for 1 epoch 0.06391215324401855 sec\n",
      "\n",
      "Epoch 113 Batch 0 Loss 0.0748\n",
      "Epoch 113 Loss 0.0771\n",
      "Time taken for 1 epoch 0.06798338890075684 sec\n",
      "\n",
      "Epoch 114 Batch 0 Loss 0.0883\n",
      "Epoch 114 Loss 0.0743\n",
      "Time taken for 1 epoch 0.06285405158996582 sec\n",
      "\n",
      "Epoch 115 Batch 0 Loss 0.0780\n",
      "Epoch 115 Loss 0.0735\n",
      "Time taken for 1 epoch 0.06287908554077148 sec\n",
      "\n",
      "Epoch 116 Batch 0 Loss 0.0911\n",
      "Epoch 116 Loss 0.0763\n",
      "Time taken for 1 epoch 0.06723690032958984 sec\n",
      "\n",
      "Epoch 117 Batch 0 Loss 0.0649\n",
      "Epoch 117 Loss 0.0740\n",
      "Time taken for 1 epoch 0.06300806999206543 sec\n",
      "\n",
      "Epoch 118 Batch 0 Loss 0.0922\n",
      "Epoch 118 Loss 0.0734\n",
      "Time taken for 1 epoch 0.06333589553833008 sec\n",
      "\n",
      "Epoch 119 Batch 0 Loss 0.0769\n",
      "Epoch 119 Loss 0.0741\n",
      "Time taken for 1 epoch 0.06334328651428223 sec\n",
      "\n",
      "Epoch 120 Batch 0 Loss 0.0854\n",
      "Epoch 120 Loss 0.0702\n",
      "Time taken for 1 epoch 0.06465339660644531 sec\n",
      "\n",
      "Epoch 121 Batch 0 Loss 0.0864\n",
      "Epoch 121 Loss 0.0717\n",
      "Time taken for 1 epoch 0.06455326080322266 sec\n",
      "\n",
      "Epoch 122 Batch 0 Loss 0.0683\n",
      "Epoch 122 Loss 0.0694\n",
      "Time taken for 1 epoch 0.06309056282043457 sec\n",
      "\n",
      "Epoch 123 Batch 0 Loss 0.0683\n",
      "Epoch 123 Loss 0.0685\n",
      "Time taken for 1 epoch 0.0628812313079834 sec\n",
      "\n",
      "Epoch 124 Batch 0 Loss 0.0812\n",
      "Epoch 124 Loss 0.0697\n",
      "Time taken for 1 epoch 0.0648655891418457 sec\n",
      "\n",
      "Epoch 125 Batch 0 Loss 0.0684\n",
      "Epoch 125 Loss 0.0665\n",
      "Time taken for 1 epoch 0.0630190372467041 sec\n",
      "\n",
      "Epoch 126 Batch 0 Loss 0.0387\n",
      "Epoch 126 Loss 0.0644\n",
      "Time taken for 1 epoch 0.06444907188415527 sec\n",
      "\n",
      "Epoch 127 Batch 0 Loss 0.0407\n",
      "Epoch 127 Loss 0.0639\n",
      "Time taken for 1 epoch 0.06413078308105469 sec\n",
      "\n",
      "Epoch 128 Batch 0 Loss 0.0530\n",
      "Epoch 128 Loss 0.0655\n",
      "Time taken for 1 epoch 0.06356263160705566 sec\n",
      "\n",
      "Epoch 129 Batch 0 Loss 0.0535\n",
      "Epoch 129 Loss 0.0638\n",
      "Time taken for 1 epoch 0.06322956085205078 sec\n",
      "\n",
      "Epoch 130 Batch 0 Loss 0.0461\n",
      "Epoch 130 Loss 0.0619\n",
      "Time taken for 1 epoch 0.06311416625976562 sec\n",
      "\n",
      "Epoch 131 Batch 0 Loss 0.0747\n",
      "Epoch 131 Loss 0.0592\n",
      "Time taken for 1 epoch 0.0647578239440918 sec\n",
      "\n",
      "Epoch 132 Batch 0 Loss 0.0452\n",
      "Epoch 132 Loss 0.0588\n",
      "Time taken for 1 epoch 0.06305170059204102 sec\n",
      "\n",
      "Epoch 133 Batch 0 Loss 0.0510\n",
      "Epoch 133 Loss 0.0596\n",
      "Time taken for 1 epoch 0.06337237358093262 sec\n",
      "\n",
      "Epoch 134 Batch 0 Loss 0.0526\n",
      "Epoch 134 Loss 0.0616\n",
      "Time taken for 1 epoch 0.06489276885986328 sec\n",
      "\n",
      "Epoch 135 Batch 0 Loss 0.0498\n",
      "Epoch 135 Loss 0.0584\n",
      "Time taken for 1 epoch 0.06354284286499023 sec\n",
      "\n",
      "Epoch 136 Batch 0 Loss 0.0541\n",
      "Epoch 136 Loss 0.0571\n",
      "Time taken for 1 epoch 0.06313323974609375 sec\n",
      "\n",
      "Epoch 137 Batch 0 Loss 0.0604\n",
      "Epoch 137 Loss 0.0585\n",
      "Time taken for 1 epoch 0.09593701362609863 sec\n",
      "\n",
      "Epoch 138 Batch 0 Loss 0.0360\n",
      "Epoch 138 Loss 0.0561\n",
      "Time taken for 1 epoch 0.06500864028930664 sec\n",
      "\n",
      "Epoch 139 Batch 0 Loss 0.0495\n",
      "Epoch 139 Loss 0.0576\n",
      "Time taken for 1 epoch 0.06496214866638184 sec\n",
      "\n",
      "Epoch 140 Batch 0 Loss 0.0474\n",
      "Epoch 140 Loss 0.0549\n",
      "Time taken for 1 epoch 0.06420397758483887 sec\n",
      "\n",
      "Epoch 141 Batch 0 Loss 0.0611\n",
      "Epoch 141 Loss 0.0581\n",
      "Time taken for 1 epoch 0.06492829322814941 sec\n",
      "\n",
      "Epoch 142 Batch 0 Loss 0.0644\n",
      "Epoch 142 Loss 0.0537\n",
      "Time taken for 1 epoch 0.0662679672241211 sec\n",
      "\n",
      "Epoch 143 Batch 0 Loss 0.0457\n",
      "Epoch 143 Loss 0.0556\n",
      "Time taken for 1 epoch 0.06861519813537598 sec\n",
      "\n",
      "Epoch 144 Batch 0 Loss 0.0456\n",
      "Epoch 144 Loss 0.0562\n",
      "Time taken for 1 epoch 0.07611894607543945 sec\n",
      "\n",
      "Epoch 145 Batch 0 Loss 0.0558\n",
      "Epoch 145 Loss 0.0544\n",
      "Time taken for 1 epoch 0.06832075119018555 sec\n",
      "\n",
      "Epoch 146 Batch 0 Loss 0.0602\n",
      "Epoch 146 Loss 0.0531\n",
      "Time taken for 1 epoch 0.07526707649230957 sec\n",
      "\n",
      "Epoch 147 Batch 0 Loss 0.0637\n",
      "Epoch 147 Loss 0.0536\n",
      "Time taken for 1 epoch 0.07655453681945801 sec\n",
      "\n",
      "Epoch 148 Batch 0 Loss 0.0464\n",
      "Epoch 148 Loss 0.0533\n",
      "Time taken for 1 epoch 0.06853222846984863 sec\n",
      "\n",
      "Epoch 149 Batch 0 Loss 0.0465\n",
      "Epoch 149 Loss 0.0525\n",
      "Time taken for 1 epoch 0.0692591667175293 sec\n",
      "\n",
      "Epoch 150 Batch 0 Loss 0.0475\n",
      "Epoch 150 Loss 0.0479\n",
      "Time taken for 1 epoch 0.07013940811157227 sec\n",
      "\n",
      "Epoch 151 Batch 0 Loss 0.0554\n",
      "Epoch 151 Loss 0.0511\n",
      "Time taken for 1 epoch 0.07267284393310547 sec\n",
      "\n",
      "Epoch 152 Batch 0 Loss 0.0406\n",
      "Epoch 152 Loss 0.0479\n",
      "Time taken for 1 epoch 0.06830906867980957 sec\n",
      "\n",
      "Epoch 153 Batch 0 Loss 0.0630\n",
      "Epoch 153 Loss 0.0492\n",
      "Time taken for 1 epoch 0.06812143325805664 sec\n",
      "\n",
      "Epoch 154 Batch 0 Loss 0.0350\n",
      "Epoch 154 Loss 0.0492\n",
      "Time taken for 1 epoch 0.06794047355651855 sec\n",
      "\n",
      "Epoch 155 Batch 0 Loss 0.0406\n",
      "Epoch 155 Loss 0.0471\n",
      "Time taken for 1 epoch 0.0645895004272461 sec\n",
      "\n",
      "Epoch 156 Batch 0 Loss 0.0549\n",
      "Epoch 156 Loss 0.0466\n",
      "Time taken for 1 epoch 0.06454253196716309 sec\n",
      "\n",
      "Epoch 157 Batch 0 Loss 0.0566\n",
      "Epoch 157 Loss 0.0490\n",
      "Time taken for 1 epoch 0.06757116317749023 sec\n",
      "\n",
      "Epoch 158 Batch 0 Loss 0.0587\n",
      "Epoch 158 Loss 0.0473\n",
      "Time taken for 1 epoch 0.0673971176147461 sec\n",
      "\n",
      "Epoch 159 Batch 0 Loss 0.0377\n",
      "Epoch 159 Loss 0.0463\n",
      "Time taken for 1 epoch 0.0656440258026123 sec\n",
      "\n",
      "Epoch 160 Batch 0 Loss 0.0427\n",
      "Epoch 160 Loss 0.0464\n",
      "Time taken for 1 epoch 0.06713199615478516 sec\n",
      "\n",
      "Epoch 161 Batch 0 Loss 0.0389\n",
      "Epoch 161 Loss 0.0428\n",
      "Time taken for 1 epoch 0.06471824645996094 sec\n",
      "\n",
      "Epoch 162 Batch 0 Loss 0.0556\n",
      "Epoch 162 Loss 0.0464\n",
      "Time taken for 1 epoch 0.06453514099121094 sec\n",
      "\n",
      "Epoch 163 Batch 0 Loss 0.0381\n",
      "Epoch 163 Loss 0.0439\n",
      "Time taken for 1 epoch 0.06426405906677246 sec\n",
      "\n",
      "Epoch 164 Batch 0 Loss 0.0408\n",
      "Epoch 164 Loss 0.0444\n",
      "Time taken for 1 epoch 0.06608033180236816 sec\n",
      "\n",
      "Epoch 165 Batch 0 Loss 0.0411\n",
      "Epoch 165 Loss 0.0448\n",
      "Time taken for 1 epoch 0.06420302391052246 sec\n",
      "\n",
      "Epoch 166 Batch 0 Loss 0.0287\n",
      "Epoch 166 Loss 0.0429\n",
      "Time taken for 1 epoch 0.06468033790588379 sec\n",
      "\n",
      "Epoch 167 Batch 0 Loss 0.0391\n",
      "Epoch 167 Loss 0.0397\n",
      "Time taken for 1 epoch 0.06743431091308594 sec\n",
      "\n",
      "Epoch 168 Batch 0 Loss 0.0458\n",
      "Epoch 168 Loss 0.0418\n",
      "Time taken for 1 epoch 0.0649106502532959 sec\n",
      "\n",
      "Epoch 169 Batch 0 Loss 0.0557\n",
      "Epoch 169 Loss 0.0407\n",
      "Time taken for 1 epoch 0.06518292427062988 sec\n",
      "\n",
      "Epoch 170 Batch 0 Loss 0.0226\n",
      "Epoch 170 Loss 0.0412\n",
      "Time taken for 1 epoch 0.06518316268920898 sec\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171 Batch 0 Loss 0.0290\n",
      "Epoch 171 Loss 0.0413\n",
      "Time taken for 1 epoch 0.06818175315856934 sec\n",
      "\n",
      "Epoch 172 Batch 0 Loss 0.0626\n",
      "Epoch 172 Loss 0.0385\n",
      "Time taken for 1 epoch 0.06525278091430664 sec\n",
      "\n",
      "Epoch 173 Batch 0 Loss 0.0296\n",
      "Epoch 173 Loss 0.0390\n",
      "Time taken for 1 epoch 0.0653834342956543 sec\n",
      "\n",
      "Epoch 174 Batch 0 Loss 0.0404\n",
      "Epoch 174 Loss 0.0409\n",
      "Time taken for 1 epoch 0.06767797470092773 sec\n",
      "\n",
      "Epoch 175 Batch 0 Loss 0.0255\n",
      "Epoch 175 Loss 0.0407\n",
      "Time taken for 1 epoch 0.07842254638671875 sec\n",
      "\n",
      "Epoch 176 Batch 0 Loss 0.0301\n",
      "Epoch 176 Loss 0.0394\n",
      "Time taken for 1 epoch 0.07813739776611328 sec\n",
      "\n",
      "Epoch 177 Batch 0 Loss 0.0353\n",
      "Epoch 177 Loss 0.0392\n",
      "Time taken for 1 epoch 0.06966280937194824 sec\n",
      "\n",
      "Epoch 178 Batch 0 Loss 0.0324\n",
      "Epoch 178 Loss 0.0401\n",
      "Time taken for 1 epoch 0.06923413276672363 sec\n",
      "\n",
      "Epoch 179 Batch 0 Loss 0.0499\n",
      "Epoch 179 Loss 0.0401\n",
      "Time taken for 1 epoch 0.06337594985961914 sec\n",
      "\n",
      "Epoch 180 Batch 0 Loss 0.0404\n",
      "Epoch 180 Loss 0.0381\n",
      "Time taken for 1 epoch 0.06393599510192871 sec\n",
      "\n",
      "Epoch 181 Batch 0 Loss 0.0494\n",
      "Epoch 181 Loss 0.0368\n",
      "Time taken for 1 epoch 0.06554031372070312 sec\n",
      "\n",
      "Epoch 182 Batch 0 Loss 0.0446\n",
      "Epoch 182 Loss 0.0378\n",
      "Time taken for 1 epoch 0.06516838073730469 sec\n",
      "\n",
      "Epoch 183 Batch 0 Loss 0.0399\n",
      "Epoch 183 Loss 0.0358\n",
      "Time taken for 1 epoch 0.06543469429016113 sec\n",
      "\n",
      "Epoch 184 Batch 0 Loss 0.0250\n",
      "Epoch 184 Loss 0.0354\n",
      "Time taken for 1 epoch 0.06490302085876465 sec\n",
      "\n",
      "Epoch 185 Batch 0 Loss 0.0428\n",
      "Epoch 185 Loss 0.0363\n",
      "Time taken for 1 epoch 0.06493759155273438 sec\n",
      "\n",
      "Epoch 186 Batch 0 Loss 0.0391\n",
      "Epoch 186 Loss 0.0363\n",
      "Time taken for 1 epoch 0.06501340866088867 sec\n",
      "\n",
      "Epoch 187 Batch 0 Loss 0.0377\n",
      "Epoch 187 Loss 0.0367\n",
      "Time taken for 1 epoch 0.06448888778686523 sec\n",
      "\n",
      "Epoch 188 Batch 0 Loss 0.0373\n",
      "Epoch 188 Loss 0.0369\n",
      "Time taken for 1 epoch 0.06557250022888184 sec\n",
      "\n",
      "Epoch 189 Batch 0 Loss 0.0273\n",
      "Epoch 189 Loss 0.0350\n",
      "Time taken for 1 epoch 0.06373286247253418 sec\n",
      "\n",
      "Epoch 190 Batch 0 Loss 0.0356\n",
      "Epoch 190 Loss 0.0353\n",
      "Time taken for 1 epoch 0.06393837928771973 sec\n",
      "\n",
      "Epoch 191 Batch 0 Loss 0.0227\n",
      "Epoch 191 Loss 0.0348\n",
      "Time taken for 1 epoch 0.06791472434997559 sec\n",
      "\n",
      "Epoch 192 Batch 0 Loss 0.0346\n",
      "Epoch 192 Loss 0.0335\n",
      "Time taken for 1 epoch 0.06333208084106445 sec\n",
      "\n",
      "Epoch 193 Batch 0 Loss 0.0208\n",
      "Epoch 193 Loss 0.0331\n",
      "Time taken for 1 epoch 0.06350111961364746 sec\n",
      "\n",
      "Epoch 194 Batch 0 Loss 0.0220\n",
      "Epoch 194 Loss 0.0331\n",
      "Time taken for 1 epoch 0.06354546546936035 sec\n",
      "\n",
      "Epoch 195 Batch 0 Loss 0.0334\n",
      "Epoch 195 Loss 0.0339\n",
      "Time taken for 1 epoch 0.06750059127807617 sec\n",
      "\n",
      "Epoch 196 Batch 0 Loss 0.0302\n",
      "Epoch 196 Loss 0.0329\n",
      "Time taken for 1 epoch 0.06355524063110352 sec\n",
      "\n",
      "Epoch 197 Batch 0 Loss 0.0328\n",
      "Epoch 197 Loss 0.0319\n",
      "Time taken for 1 epoch 0.06349015235900879 sec\n",
      "\n",
      "Epoch 198 Batch 0 Loss 0.0206\n",
      "Epoch 198 Loss 0.0333\n",
      "Time taken for 1 epoch 0.06861186027526855 sec\n",
      "\n",
      "Epoch 199 Batch 0 Loss 0.0296\n",
      "Epoch 199 Loss 0.0316\n",
      "Time taken for 1 epoch 0.06587362289428711 sec\n",
      "\n",
      "Epoch 200 Batch 0 Loss 0.0302\n",
      "Epoch 200 Loss 0.0322\n",
      "Time taken for 1 epoch 0.06638097763061523 sec\n",
      "\n",
      "Epoch 201 Batch 0 Loss 0.0422\n",
      "Epoch 201 Loss 0.0313\n",
      "Time taken for 1 epoch 0.06628012657165527 sec\n",
      "\n",
      "Epoch 202 Batch 0 Loss 0.0424\n",
      "Epoch 202 Loss 0.0324\n",
      "Time taken for 1 epoch 0.06565141677856445 sec\n",
      "\n",
      "Epoch 203 Batch 0 Loss 0.0313\n",
      "Epoch 203 Loss 0.0304\n",
      "Time taken for 1 epoch 0.0640103816986084 sec\n",
      "\n",
      "Epoch 204 Batch 0 Loss 0.0414\n",
      "Epoch 204 Loss 0.0309\n",
      "Time taken for 1 epoch 0.06371450424194336 sec\n",
      "\n",
      "Epoch 205 Batch 0 Loss 0.0381\n",
      "Epoch 205 Loss 0.0292\n",
      "Time taken for 1 epoch 0.06546497344970703 sec\n",
      "\n",
      "Epoch 206 Batch 0 Loss 0.0210\n",
      "Epoch 206 Loss 0.0287\n",
      "Time taken for 1 epoch 0.06371569633483887 sec\n",
      "\n",
      "Epoch 207 Batch 0 Loss 0.0312\n",
      "Epoch 207 Loss 0.0300\n",
      "Time taken for 1 epoch 0.06565284729003906 sec\n",
      "\n",
      "Epoch 208 Batch 0 Loss 0.0193\n",
      "Epoch 208 Loss 0.0308\n",
      "Time taken for 1 epoch 0.06469917297363281 sec\n",
      "\n",
      "Epoch 209 Batch 0 Loss 0.0475\n",
      "Epoch 209 Loss 0.0301\n",
      "Time taken for 1 epoch 0.06979942321777344 sec\n",
      "\n",
      "Epoch 210 Batch 0 Loss 0.0330\n",
      "Epoch 210 Loss 0.0304\n",
      "Time taken for 1 epoch 0.0652153491973877 sec\n",
      "\n",
      "Epoch 211 Batch 0 Loss 0.0234\n",
      "Epoch 211 Loss 0.0297\n",
      "Time taken for 1 epoch 0.06550073623657227 sec\n",
      "\n",
      "Epoch 212 Batch 0 Loss 0.0279\n",
      "Epoch 212 Loss 0.0294\n",
      "Time taken for 1 epoch 0.06739568710327148 sec\n",
      "\n",
      "Epoch 213 Batch 0 Loss 0.0156\n",
      "Epoch 213 Loss 0.0295\n",
      "Time taken for 1 epoch 0.06636571884155273 sec\n",
      "\n",
      "Epoch 214 Batch 0 Loss 0.0128\n",
      "Epoch 214 Loss 0.0286\n",
      "Time taken for 1 epoch 0.0650780200958252 sec\n",
      "\n",
      "Epoch 215 Batch 0 Loss 0.0276\n",
      "Epoch 215 Loss 0.0292\n",
      "Time taken for 1 epoch 0.06589841842651367 sec\n",
      "\n",
      "Epoch 216 Batch 0 Loss 0.0363\n",
      "Epoch 216 Loss 0.0281\n",
      "Time taken for 1 epoch 0.06605648994445801 sec\n",
      "\n",
      "Epoch 217 Batch 0 Loss 0.0208\n",
      "Epoch 217 Loss 0.0279\n",
      "Time taken for 1 epoch 0.06683588027954102 sec\n",
      "\n",
      "Epoch 218 Batch 0 Loss 0.0248\n",
      "Epoch 218 Loss 0.0272\n",
      "Time taken for 1 epoch 0.06449723243713379 sec\n",
      "\n",
      "Epoch 219 Batch 0 Loss 0.0356\n",
      "Epoch 219 Loss 0.0279\n",
      "Time taken for 1 epoch 0.06665205955505371 sec\n",
      "\n",
      "Epoch 220 Batch 0 Loss 0.0307\n",
      "Epoch 220 Loss 0.0278\n",
      "Time taken for 1 epoch 0.06638574600219727 sec\n",
      "\n",
      "Epoch 221 Batch 0 Loss 0.0303\n",
      "Epoch 221 Loss 0.0272\n",
      "Time taken for 1 epoch 0.06556248664855957 sec\n",
      "\n",
      "Epoch 222 Batch 0 Loss 0.0202\n",
      "Epoch 222 Loss 0.0265\n",
      "Time taken for 1 epoch 0.06524777412414551 sec\n",
      "\n",
      "Epoch 223 Batch 0 Loss 0.0203\n",
      "Epoch 223 Loss 0.0260\n",
      "Time taken for 1 epoch 0.06633782386779785 sec\n",
      "\n",
      "Epoch 224 Batch 0 Loss 0.0233\n",
      "Epoch 224 Loss 0.0270\n",
      "Time taken for 1 epoch 0.06432175636291504 sec\n",
      "\n",
      "Epoch 225 Batch 0 Loss 0.0402\n",
      "Epoch 225 Loss 0.0256\n",
      "Time taken for 1 epoch 0.06418895721435547 sec\n",
      "\n",
      "Epoch 226 Batch 0 Loss 0.0292\n",
      "Epoch 226 Loss 0.0251\n",
      "Time taken for 1 epoch 0.06561017036437988 sec\n",
      "\n",
      "Epoch 227 Batch 0 Loss 0.0216\n",
      "Epoch 227 Loss 0.0245\n",
      "Time taken for 1 epoch 0.0643911361694336 sec\n",
      "\n",
      "Epoch 228 Batch 0 Loss 0.0215\n",
      "Epoch 228 Loss 0.0260\n",
      "Time taken for 1 epoch 0.06411504745483398 sec\n",
      "\n",
      "Epoch 229 Batch 0 Loss 0.0148\n",
      "Epoch 229 Loss 0.0253\n",
      "Time taken for 1 epoch 0.0639801025390625 sec\n",
      "\n",
      "Epoch 230 Batch 0 Loss 0.0233\n",
      "Epoch 230 Loss 0.0267\n",
      "Time taken for 1 epoch 0.06572937965393066 sec\n",
      "\n",
      "Epoch 231 Batch 0 Loss 0.0395\n",
      "Epoch 231 Loss 0.0242\n",
      "Time taken for 1 epoch 0.06459784507751465 sec\n",
      "\n",
      "Epoch 232 Batch 0 Loss 0.0322\n",
      "Epoch 232 Loss 0.0261\n",
      "Time taken for 1 epoch 0.0638113021850586 sec\n",
      "\n",
      "Epoch 233 Batch 0 Loss 0.0123\n",
      "Epoch 233 Loss 0.0258\n",
      "Time taken for 1 epoch 0.06424641609191895 sec\n",
      "\n",
      "Epoch 234 Batch 0 Loss 0.0327\n",
      "Epoch 234 Loss 0.0242\n",
      "Time taken for 1 epoch 0.06412339210510254 sec\n",
      "\n",
      "Epoch 235 Batch 0 Loss 0.0157\n",
      "Epoch 235 Loss 0.0239\n",
      "Time taken for 1 epoch 0.0639352798461914 sec\n",
      "\n",
      "Epoch 236 Batch 0 Loss 0.0145\n",
      "Epoch 236 Loss 0.0241\n",
      "Time taken for 1 epoch 0.06407928466796875 sec\n",
      "\n",
      "Epoch 237 Batch 0 Loss 0.0334\n",
      "Epoch 237 Loss 0.0238\n",
      "Time taken for 1 epoch 0.06531620025634766 sec\n",
      "\n",
      "Epoch 238 Batch 0 Loss 0.0143\n",
      "Epoch 238 Loss 0.0239\n",
      "Time taken for 1 epoch 0.0641489028930664 sec\n",
      "\n",
      "Epoch 239 Batch 0 Loss 0.0203\n",
      "Epoch 239 Loss 0.0245\n",
      "Time taken for 1 epoch 0.06375432014465332 sec\n",
      "\n",
      "Epoch 240 Batch 0 Loss 0.0207\n",
      "Epoch 240 Loss 0.0245\n",
      "Time taken for 1 epoch 0.06579208374023438 sec\n",
      "\n",
      "Epoch 241 Batch 0 Loss 0.0174\n",
      "Epoch 241 Loss 0.0244\n",
      "Time taken for 1 epoch 0.06789660453796387 sec\n",
      "\n",
      "Epoch 242 Batch 0 Loss 0.0321\n",
      "Epoch 242 Loss 0.0241\n",
      "Time taken for 1 epoch 0.06443238258361816 sec\n",
      "\n",
      "Epoch 243 Batch 0 Loss 0.0262\n",
      "Epoch 243 Loss 0.0231\n",
      "Time taken for 1 epoch 0.06546592712402344 sec\n",
      "\n",
      "Epoch 244 Batch 0 Loss 0.0159\n",
      "Epoch 244 Loss 0.0236\n",
      "Time taken for 1 epoch 0.0656588077545166 sec\n",
      "\n",
      "Epoch 245 Batch 0 Loss 0.0182\n",
      "Epoch 245 Loss 0.0221\n",
      "Time taken for 1 epoch 0.06372404098510742 sec\n",
      "\n",
      "Epoch 246 Batch 0 Loss 0.0235\n",
      "Epoch 246 Loss 0.0216\n",
      "Time taken for 1 epoch 0.06348299980163574 sec\n",
      "\n",
      "Epoch 247 Batch 0 Loss 0.0184\n",
      "Epoch 247 Loss 0.0232\n",
      "Time taken for 1 epoch 0.06652045249938965 sec\n",
      "\n",
      "Epoch 248 Batch 0 Loss 0.0244\n",
      "Epoch 248 Loss 0.0232\n",
      "Time taken for 1 epoch 0.06416559219360352 sec\n",
      "\n",
      "Epoch 249 Batch 0 Loss 0.0208\n",
      "Epoch 249 Loss 0.0216\n",
      "Time taken for 1 epoch 0.06355690956115723 sec\n",
      "\n",
      "Epoch 250 Batch 0 Loss 0.0170\n",
      "Epoch 250 Loss 0.0232\n",
      "Time taken for 1 epoch 0.06358456611633301 sec\n",
      "\n",
      "Epoch 251 Batch 0 Loss 0.0322\n",
      "Epoch 251 Loss 0.0210\n",
      "Time taken for 1 epoch 0.06573724746704102 sec\n",
      "\n",
      "Epoch 252 Batch 0 Loss 0.0252\n",
      "Epoch 252 Loss 0.0228\n",
      "Time taken for 1 epoch 0.06354713439941406 sec\n",
      "\n",
      "Epoch 253 Batch 0 Loss 0.0247\n",
      "Epoch 253 Loss 0.0206\n",
      "Time taken for 1 epoch 0.06377267837524414 sec\n",
      "\n",
      "Epoch 254 Batch 0 Loss 0.0213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 254 Loss 0.0225\n",
      "Time taken for 1 epoch 0.08187651634216309 sec\n",
      "\n",
      "Epoch 255 Batch 0 Loss 0.0253\n",
      "Epoch 255 Loss 0.0217\n",
      "Time taken for 1 epoch 0.06709003448486328 sec\n",
      "\n",
      "Epoch 256 Batch 0 Loss 0.0182\n",
      "Epoch 256 Loss 0.0222\n",
      "Time taken for 1 epoch 0.06466889381408691 sec\n",
      "\n",
      "Epoch 257 Batch 0 Loss 0.0221\n",
      "Epoch 257 Loss 0.0217\n",
      "Time taken for 1 epoch 0.06440401077270508 sec\n",
      "\n",
      "Epoch 258 Batch 0 Loss 0.0113\n",
      "Epoch 258 Loss 0.0218\n",
      "Time taken for 1 epoch 0.06566095352172852 sec\n",
      "\n",
      "Epoch 259 Batch 0 Loss 0.0261\n",
      "Epoch 259 Loss 0.0215\n",
      "Time taken for 1 epoch 0.06453943252563477 sec\n",
      "\n",
      "Epoch 260 Batch 0 Loss 0.0109\n",
      "Epoch 260 Loss 0.0215\n",
      "Time taken for 1 epoch 0.06446719169616699 sec\n",
      "\n",
      "Epoch 261 Batch 0 Loss 0.0139\n",
      "Epoch 261 Loss 0.0214\n",
      "Time taken for 1 epoch 0.06526613235473633 sec\n",
      "\n",
      "Epoch 262 Batch 0 Loss 0.0157\n",
      "Epoch 262 Loss 0.0214\n",
      "Time taken for 1 epoch 0.06854724884033203 sec\n",
      "\n",
      "Epoch 263 Batch 0 Loss 0.0221\n",
      "Epoch 263 Loss 0.0201\n",
      "Time taken for 1 epoch 0.06593203544616699 sec\n",
      "\n",
      "Epoch 264 Batch 0 Loss 0.0137\n",
      "Epoch 264 Loss 0.0207\n",
      "Time taken for 1 epoch 0.06849527359008789 sec\n",
      "\n",
      "Epoch 265 Batch 0 Loss 0.0148\n",
      "Epoch 265 Loss 0.0212\n",
      "Time taken for 1 epoch 0.06599116325378418 sec\n",
      "\n",
      "Epoch 266 Batch 0 Loss 0.0105\n",
      "Epoch 266 Loss 0.0193\n",
      "Time taken for 1 epoch 0.06500911712646484 sec\n",
      "\n",
      "Epoch 267 Batch 0 Loss 0.0110\n",
      "Epoch 267 Loss 0.0190\n",
      "Time taken for 1 epoch 0.06484079360961914 sec\n",
      "\n",
      "Epoch 268 Batch 0 Loss 0.0300\n",
      "Epoch 268 Loss 0.0199\n",
      "Time taken for 1 epoch 0.06687641143798828 sec\n",
      "\n",
      "Epoch 269 Batch 0 Loss 0.0226\n",
      "Epoch 269 Loss 0.0211\n",
      "Time taken for 1 epoch 0.06687355041503906 sec\n",
      "\n",
      "Epoch 270 Batch 0 Loss 0.0278\n",
      "Epoch 270 Loss 0.0208\n",
      "Time taken for 1 epoch 0.06660628318786621 sec\n",
      "\n",
      "Epoch 271 Batch 0 Loss 0.0173\n",
      "Epoch 271 Loss 0.0203\n",
      "Time taken for 1 epoch 0.06821393966674805 sec\n",
      "\n",
      "Epoch 272 Batch 0 Loss 0.0171\n",
      "Epoch 272 Loss 0.0207\n",
      "Time taken for 1 epoch 0.06796813011169434 sec\n",
      "\n",
      "Epoch 273 Batch 0 Loss 0.0209\n",
      "Epoch 273 Loss 0.0202\n",
      "Time taken for 1 epoch 0.06659460067749023 sec\n",
      "\n",
      "Epoch 274 Batch 0 Loss 0.0129\n",
      "Epoch 274 Loss 0.0188\n",
      "Time taken for 1 epoch 0.06621122360229492 sec\n",
      "\n",
      "Epoch 275 Batch 0 Loss 0.0177\n",
      "Epoch 275 Loss 0.0201\n",
      "Time taken for 1 epoch 0.06902670860290527 sec\n",
      "\n",
      "Epoch 276 Batch 0 Loss 0.0178\n",
      "Epoch 276 Loss 0.0201\n",
      "Time taken for 1 epoch 0.06691241264343262 sec\n",
      "\n",
      "Epoch 277 Batch 0 Loss 0.0296\n",
      "Epoch 277 Loss 0.0200\n",
      "Time taken for 1 epoch 0.06674027442932129 sec\n",
      "\n",
      "Epoch 278 Batch 0 Loss 0.0246\n",
      "Epoch 278 Loss 0.0193\n",
      "Time taken for 1 epoch 0.06865453720092773 sec\n",
      "\n",
      "Epoch 279 Batch 0 Loss 0.0255\n",
      "Epoch 279 Loss 0.0198\n",
      "Time taken for 1 epoch 0.06692242622375488 sec\n",
      "\n",
      "Epoch 280 Batch 0 Loss 0.0132\n",
      "Epoch 280 Loss 0.0191\n",
      "Time taken for 1 epoch 0.06633138656616211 sec\n",
      "\n",
      "Epoch 281 Batch 0 Loss 0.0179\n",
      "Epoch 281 Loss 0.0196\n",
      "Time taken for 1 epoch 0.06847763061523438 sec\n",
      "\n",
      "Epoch 282 Batch 0 Loss 0.0098\n",
      "Epoch 282 Loss 0.0196\n",
      "Time taken for 1 epoch 0.06891059875488281 sec\n",
      "\n",
      "Epoch 283 Batch 0 Loss 0.0286\n",
      "Epoch 283 Loss 0.0194\n",
      "Time taken for 1 epoch 0.06829595565795898 sec\n",
      "\n",
      "Epoch 284 Batch 0 Loss 0.0215\n",
      "Epoch 284 Loss 0.0195\n",
      "Time taken for 1 epoch 0.06614160537719727 sec\n",
      "\n",
      "Epoch 285 Batch 0 Loss 0.0077\n",
      "Epoch 285 Loss 0.0189\n",
      "Time taken for 1 epoch 0.06546401977539062 sec\n",
      "\n",
      "Epoch 286 Batch 0 Loss 0.0377\n",
      "Epoch 286 Loss 0.0191\n",
      "Time taken for 1 epoch 0.06513833999633789 sec\n",
      "\n",
      "Epoch 287 Batch 0 Loss 0.0095\n",
      "Epoch 287 Loss 0.0189\n",
      "Time taken for 1 epoch 0.06651687622070312 sec\n",
      "\n",
      "Epoch 288 Batch 0 Loss 0.0257\n",
      "Epoch 288 Loss 0.0191\n",
      "Time taken for 1 epoch 0.06636357307434082 sec\n",
      "\n",
      "Epoch 289 Batch 0 Loss 0.0174\n",
      "Epoch 289 Loss 0.0174\n",
      "Time taken for 1 epoch 0.06882309913635254 sec\n",
      "\n",
      "Epoch 290 Batch 0 Loss 0.0122\n",
      "Epoch 290 Loss 0.0160\n",
      "Time taken for 1 epoch 0.06870579719543457 sec\n",
      "\n",
      "Epoch 291 Batch 0 Loss 0.0121\n",
      "Epoch 291 Loss 0.0182\n",
      "Time taken for 1 epoch 0.07292294502258301 sec\n",
      "\n",
      "Epoch 292 Batch 0 Loss 0.0141\n",
      "Epoch 292 Loss 0.0183\n",
      "Time taken for 1 epoch 0.07569551467895508 sec\n",
      "\n",
      "Epoch 293 Batch 0 Loss 0.0180\n",
      "Epoch 293 Loss 0.0181\n",
      "Time taken for 1 epoch 0.07137274742126465 sec\n",
      "\n",
      "Epoch 294 Batch 0 Loss 0.0099\n",
      "Epoch 294 Loss 0.0183\n",
      "Time taken for 1 epoch 0.07993960380554199 sec\n",
      "\n",
      "Epoch 295 Batch 0 Loss 0.0264\n",
      "Epoch 295 Loss 0.0171\n",
      "Time taken for 1 epoch 0.07103562355041504 sec\n",
      "\n",
      "Epoch 296 Batch 0 Loss 0.0111\n",
      "Epoch 296 Loss 0.0159\n",
      "Time taken for 1 epoch 0.07280278205871582 sec\n",
      "\n",
      "Epoch 297 Batch 0 Loss 0.0136\n",
      "Epoch 297 Loss 0.0182\n",
      "Time taken for 1 epoch 0.0726327896118164 sec\n",
      "\n",
      "Epoch 298 Batch 0 Loss 0.0092\n",
      "Epoch 298 Loss 0.0183\n",
      "Time taken for 1 epoch 0.07077932357788086 sec\n",
      "\n",
      "Epoch 299 Batch 0 Loss 0.0133\n",
      "Epoch 299 Loss 0.0179\n",
      "Time taken for 1 epoch 0.07601046562194824 sec\n",
      "\n",
      "Epoch 300 Batch 0 Loss 0.0184\n",
      "Epoch 300 Loss 0.0178\n",
      "Time taken for 1 epoch 0.06638288497924805 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 300\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch, (inp, targ, inp_len)) in enumerate(dataset):\n",
    "        loss = 0\n",
    "        \n",
    "        xs, ys, lens = sort_batch(inp, targ, inp_len)\n",
    "        enc_output, enc_hidden = encoder(xs.to(device), lens, device=device)\n",
    "        dec_hidden = enc_hidden\n",
    "        \n",
    "        # use teacher forcing - feeding the target as the next input (via dec_input)\n",
    "        dec_input = torch.tensor([[targ_index.word2idx['<start>']]] * BATCH_SIZE)\n",
    "        \n",
    "        # run code below for every timestep in the ys batch\n",
    "        for t in range(1, ys.size(1)):\n",
    "            predictions, dec_hidden, _ = decoder(dec_input.to(device), \n",
    "                                         dec_hidden.to(device), \n",
    "                                         enc_output.to(device))\n",
    "            loss += loss_function(ys[:, t].to(device), predictions.to(device))\n",
    "            #loss += loss_\n",
    "            dec_input = ys[:, t].unsqueeze(1)\n",
    "            \n",
    "        \n",
    "        batch_loss = (loss / int(ys.size(1)))\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "        \n",
    "        if not batch % 100:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.detach().item()))\n",
    "        \n",
    "        \n",
    "    ### TODO: Save checkpoint for model\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                        total_loss / N_BATCH))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   5, 5697, 5449,  ...,    0,    0,    0],\n",
      "        [   5, 3873,   69,  ...,    0,    0,    0],\n",
      "        [   5, 5697, 3078,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [   5, 9334, 7298,  ...,    0,    0,    0],\n",
      "        [   5, 3267, 4244,  ...,    0,    0,    0],\n",
      "        [   5, 3267, 6040,  ...,    0,    0,    0]])\n",
      "tensor([[   5, 2124, 2311, 2551, 2269, 4530,    3,    4,    0,    0,    0],\n",
      "        [   5, 2124, 2549,   23, 4138,    1,    4,    0,    0,    0,    0],\n",
      "        [   5, 1221, 2269, 1187, 2603,    3,    4,    0,    0,    0,    0],\n",
      "        [   5, 4288, 4552, 1824,    3,    4,    0,    0,    0,    0,    0],\n",
      "        [   5, 4805, 3942, 4144,    3,    4,    0,    0,    0,    0,    0],\n",
      "        [   5, 4698, 2520,    6,    4,    0,    0,    0,    0,    0,    0],\n",
      "        [   5, 4013, 3804,    3,    4,    0,    0,    0,    0,    0,    0],\n",
      "        [   5, 2124, 3918, 4278, 3314,    3,    4,    0,    0,    0,    0],\n",
      "        [   5, 2124,   59, 4362,    3,    4,    0,    0,    0,    0,    0],\n",
      "        [   5, 4362, 2264, 2956, 2893, 4756,    3,    4,    0,    0,    0],\n",
      "        [   5, 2826, 2916, 4621, 4286,    3,    4,    0,    0,    0,    0],\n",
      "        [   5,  726, 4808, 2172,    3,    4,    0,    0,    0,    0,    0],\n",
      "        [   5, 4643, 2033,    7, 1073,    3,    4,    0,    0,    0,    0],\n",
      "        [   5, 4805, 3371, 3589,    3,    4,    0,    0,    0,    0,    0],\n",
      "        [   5, 2264, 4304, 4808, 1522,    6,    4,    0,    0,    0,    0],\n",
      "        [   5, 4643,  628, 4186, 1993, 4362,    3,    4,    0,    0,    0],\n",
      "        [   5, 2124, 2791, 2709, 3221,    3,    4,    0,    0,    0,    0],\n",
      "        [   5, 3160, 3814, 1242,    3,    4,    0,    0,    0,    0,    0],\n",
      "        [   5, 4643,  628, 4186, 4714,    3,    4,    0,    0,    0,    0],\n",
      "        [   5, 4443, 2847, 4352, 4790,    3,    4,    0,    0,    0,    0],\n",
      "        [   5, 3736, 3472,    7, 3551,    3,    4,    0,    0,    0,    0],\n",
      "        [   5, 4362, 3296,    7, 2747,    3,    4,    0,    0,    0,    0],\n",
      "        [   5, 3765, 4643, 4053,    6,    4,    0,    0,    0,    0,    0],\n",
      "        [   5, 4698, 4621, 4277,    6,    4,    0,    0,    0,    0,    0],\n",
      "        [   5, 1961, 4787, 2269, 2109,    3,    4,    0,    0,    0,    0],\n",
      "        [   5, 2124, 4552, 2520, 2758, 4609,    3,    4,    0,    0,    0],\n",
      "        [   5, 2124, 2359, 4304, 2264, 4094,    3,    4,    0,    0,    0],\n",
      "        [   5, 3736, 3584, 2914,    7, 1142,    3,    4,    0,    0,    0],\n",
      "        [   5, 1815, 2551, 4278, 4507,    3,    4,    0,    0,    0,    0],\n",
      "        [   5, 2124, 3357, 2238,    7, 4415,    3,    4,    0,    0,    0],\n",
      "        [   5, 1961, 3584,    7, 2398,    3,    4,    0,    0,    0,    0],\n",
      "        [   5, 1985,  205, 1017,    3,    4,    0,    0,    0,    0,    0],\n",
      "        [   5, 4362, 1945, 1824, 1608,    3,    4,    0,    0,    0,    0],\n",
      "        [   5, 2124, 2549, 1935,    3,    4,    0,    0,    0,    0,    0],\n",
      "        [   5, 4805,  205, 4094,    3,    4,    0,    0,    0,    0,    0],\n",
      "        [   5, 4643,  628, 4186, 1810, 2048, 4802,    3,    4,    0,    0],\n",
      "        [   5, 4362, 2264, 2000,    3,    4,    0,    0,    0,    0,    0],\n",
      "        [   5, 2124, 1696, 4362,    3,    4,    0,    0,    0,    0,    0],\n",
      "        [   5, 4362, 4623, 4278,  650,    3,    4,    0,    0,    0,    0],\n",
      "        [   5, 2124, 2490, 4629, 2269,    3,    4,    0,    0,    0,    0],\n",
      "        [   5, 1925, 2914,    3,    4,    0,    0,    0,    0,    0,    0],\n",
      "        [   5, 1961, 1767, 1999,    7,  484,    3,    4,    0,    0,    0],\n",
      "        [   5, 2009,    2,  842, 2914, 2171,    3,    4,    0,    0,    0],\n",
      "        [   5, 1138, 4805, 3625, 4800,    6,    4,    0,    0,    0,    0],\n",
      "        [   5, 4278,  295, 3584,  283,    3,    4,    0,    0,    0,    0],\n",
      "        [   5, 2124, 4747, 4278, 2523,    3,    4,    0,    0,    0,    0],\n",
      "        [   5, 1961, 2419,    7, 2666,   91,    3,    4,    0,    0,    0],\n",
      "        [   5, 2124, 2456,  739,    3,    4,    0,    0,    0,    0,    0],\n",
      "        [   5, 2269, 3584,    7, 2639,    3,    4,    0,    0,    0,    0],\n",
      "        [   5, 2264, 4277, 3305, 1820,    6,    4,    0,    0,    0,    0],\n",
      "        [   5, 2857, 1329, 4808, 4133,    3,    4,    0,    0,    0,    0],\n",
      "        [   5,  132, 2124, 1819, 4352, 1140,    6,    4,    0,    0,    0],\n",
      "        [   5, 4237, 2603, 1434,    3,    4,    0,    0,    0,    0,    0],\n",
      "        [   5, 2264, 2269, 2000,    6,    4,    0,    0,    0,    0,    0],\n",
      "        [   5,  347, 3482,    3,    4,    0,    0,    0,    0,    0,    0],\n",
      "        [   5, 1433, 2007,    3,    4,    0,    0,    0,    0,    0,    0],\n",
      "        [   5, 4362, 3584, 3551, 2264, 1379,    3,    4,    0,    0,    0],\n",
      "        [   5, 2124, 2549,    7,  943,  517, 2857,    3,    4,    0,    0],\n",
      "        [   5, 2922, 1601,    1,    4,    0,    0,    0,    0,    0,    0],\n",
      "        [   5, 2758, 1661, 1563,  241,    3,    4,    0,    0,    0,    0],\n",
      "        [   5, 4805, 2754, 2110, 4530,    3,    4,    0,    0,    0,    0],\n",
      "        [   5, 4681, 3584, 4277, 4286,    6,    4,    0,    0,    0,    0],\n",
      "        [   5, 1961, 4621, 2167,    3,    4,    0,    0,    0,    0,    0],\n",
      "        [   5, 4681,    7, 3862, 4764,    1,    4,    0,    0,    0,    0]])\n",
      "tensor([8, 6, 6, 6, 5, 6, 6, 6, 6, 6, 6, 8, 8, 5, 8, 8, 6, 6, 6, 6, 6, 8, 5, 7,\n",
      "        8, 6, 8, 7, 7, 7, 6, 7, 8, 5, 5, 9, 6, 7, 7, 5, 5, 8, 7, 7, 7, 7, 9, 7,\n",
      "        6, 7, 8, 8, 5, 5, 5, 5, 9, 9, 4, 8, 7, 7, 6, 8])\n"
     ]
    }
   ],
   "source": [
    "for (inp, targ, inp_len) in dataset:\n",
    "    break\n",
    "print(inp)\n",
    "print(targ)\n",
    "print(inp_len)\n",
    "#xs, ys, lens = sort_batch(inp, targ, inp_len)\n",
    "#enc_output, enc_hidden = encoder(xs.to(device), lens, device=device)\n",
    "#dec_hidden = enc_hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spanish_to_english(english_sentence):\n",
    "    english_tensor = [inp_index.word2idx[w] for w in english_sentence.split()]\n",
    "    inp_len = len(english_tensor)\n",
    "    # xs, ys, lens = sort_batch(inp, targ, inp_len)\n",
    "    enc_output, enc_hidden = encoder(torch.tensor([english_tensor]*64).to(device), [inp_len]*64, device=device)\n",
    "    \n",
    "    # encode(english_sentence)\n",
    "    return enc_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected hidden size (1, 6, 1024), got [1, 64, 1024]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-ccaff8440599>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspanish_to_english\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<start> como estas usted ? <end>'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-69-f6b8a56f3746>\u001b[0m in \u001b[0;36mspanish_to_english\u001b[0;34m(english_sentence)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0minp_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menglish_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# xs, ys, lens = sort_batch(inp, targ, inp_len)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0menc_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menglish_tensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minp_len\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# encode(english_sentence)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/qaryenv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-34af7de8141e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, lens, device)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# output: max_length, batch_size, enc_units\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# self.hidden: 1, batch_size, enc_units\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# gru returns hidden state of all timesteps as well as hidden state at last timestep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# pad the sequence to the max length in the batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/qaryenv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/qaryenv/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    735\u001b[0m             \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    738\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m             result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "\u001b[0;32m~/anaconda3/envs/qaryenv/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mexpected_hidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_expected_hidden_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_hidden_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpermutation\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/qaryenv/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_hidden_size\u001b[0;34m(self, hx, expected_hidden_size, msg)\u001b[0m\n\u001b[1;32m    194\u001b[0m                           msg: str = 'Expected hidden size {}, got {}') -> None:\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected hidden size (1, 6, 1024), got [1, 64, 1024]"
     ]
    }
   ],
   "source": [
    "spanish_to_english('<start> como estas usted ? <end>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_tF5jMP0-cmv"
   },
   "source": [
    "## Final Words\n",
    "Notice that we only trained the model and that's it. In fact, this notebook is in experimental phase, so there could also be some bugs or something I missed during the process of converting code or training. Please comment your concerns here or submit it as an issue in the [GitHub version](https://github.com/omarsar/pytorch_neural_machine_translation_attention) of this notebook. I will appreciate it!\n",
    "\n",
    "We didn't evaluate the model or analyzed it. To encourage you to practice what you have learned in the notebook, I will suggest that you try to convert the TensorFlow code used in the [original notebook](https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/nmt_with_attention/nmt_with_attention.ipynb) and complete this notebook. I believe the code should be straightforward, the hard part was already done in this notebook. If you manage to complete it, please submit a PR on the GitHub version of this notebook. I will gladly accept your PR. Thanks for reading and hope this notebook was useful. Keep tuned for notebooks like this on my Twitter ([omarsar0](https://twitter.com/omarsar0)). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cl4ZgMd-KyTU"
   },
   "source": [
    "## References\n",
    "\n",
    "### Seq2Seq:\n",
    "  - Sutskever et al. (2014) - [Sequence to Sequence Learning with Neural Networks](Sequence to Sequence Learning with Neural Networks)\n",
    "  - [Sequence to sequence model: Introduction and concepts](https://towardsdatascience.com/sequence-to-sequence-model-introduction-and-concepts-44d9b41cd42d)\n",
    "  - [Blog on seq2seq](https://guillaumegenthial.github.io/sequence-to-sequence.html)\n",
    "  - [Bahdanau et al. (2016) NMT jointly learning to align and translate](https://arxiv.org/pdf/1409.0473.pdf)\n",
    "  - [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NMT in PyTorch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
