# -*- coding: utf-8 -*-
from setuptools import setup

packages = \
['efax', 'efax.scipy_replacement']

package_data = \
{'': ['*']}

install_requires = \
['chex>=0,<1',
 'jax>=0.2,<0.3',
 'jaxlib>=0.1.55,<0.2.0',
 'numpy>=1.20,<1.21',
 'scipy>=1.4,<2.0',
 'tensorflow-probability>=0.12,<0.13',
 'tjax>=0.7.13,<1.0']

setup_kwargs = {
    'name': 'efax',
    'version': '1.2.1',
    'description': 'Exponential families for JAX',
    'long_description': "=================================\nEFAX: Exponential Families in JAX\n=================================\n.. image:: https://badge.fury.io/py/efax.svg\n    :target: https://badge.fury.io/py/efax\n\n.. role:: bash(code)\n    :language: bash\n\n.. role:: python(code)\n   :language: python\n\nThis library provides a set of tools for working with *exponential family distributions* in the\ndifferential programming library `JAX <https://github.com/google/jax/>`_.\nThe *exponential families* are an important class of probability distributions that include the\nnormal, gamma, beta, exponential, Poisson, binomial, and Bernoulli distributions.\nFor an explanation of the fundamental ideas behind this library, see our `overview on exponential\nfamilies <https://github.com/NeilGirdhar/efax/blob/master/expfam.pdf>`_.\n\nFramework\n=========\nRepresentation\n--------------\nEFAX has a single base class for its objects: :python:`Parametrization` that encodes the\ndistribution family, and the parameters of the distribution.\nEach such object has a shape, and so it can store any number of distributions.\nWhen operating on such objects, NumPy's broadcasting rules apply.\nThis is unlike SciPy where each distribution is represented by a single object, and so a thousand\ndistributions need a thousand objects.\n\nAll parametrization objects are dataclasses using :python:`tjax.dataclass`.  These dataclasses are\na modification of Python's dataclasses to support JAX's type registration.  This allows easy marking\nof static attributes.  In JAX, a static attribute is one that induces recompilation of a function\nwhen it changes, and consequently there is more flexibility about what can be done with such an\nattribute.  An example of a static attribute in EFAX is the failure number of the negative binomial\ndistribution.\n\nEach non-static attribute in an EFAX distribution is marked with a support.  For example:\n\n.. code:: python\n\n    @dataclass\n    class MultivariateNormalNP(NaturalParametrization['MultivariateNormalEP']):\n        mean_times_precision: RealArray = distribution_parameter(VectorSupport())\n        negative_half_precision: RealArray = distribution_parameter(SymmetricMatrixSupport())\n\nIn this case, we see that there are two natural parameters for the multivariate normal distribution.\nIf such an object :python:`x` has shape :python:`s`, then the shape of\n:python:`x.negative_half_precision` is :python:`(*s, n, n)`.\n\nParametrizations\n----------------\nEach exponential family distribution has two special parametrizations: the natural and the\nexpectation parametrization.  (These are described in the overview pdf.)\nConsequently, every distribution has two base classes, one inheriting from\n:python:`NaturalParametrization` and one from :python:`ExpectationParametrization`.\n\nThe motivation for the natural parametrization is combining and scaling independent predictive\nevidence.  In the natural parametrization, these operations correspond to scaling and addition.\n\nThe motivation for the expectation parametrization is combining independent observations into the\nmaximum likelihood distribution that could have produced them.  In the expectation parametrization,\nthis is an expected value.\n\nEFAX provides conversions between the two parametrizations through the\n:python:`NaturalParametrization.to_exp` and :python:`ExpectationParametrization.to_nat` methods.\n\nImportant methods\n-----------------\nEvery :python:`Parametrization` has methods to flatten and unflatten the parameters into a single\narray: :python:`flattened` and :python:`unflattened`.\n\nEvery :python:`NaturalParametrization` has methods:\n\n- :python:`sufficient_statistics` to produce the sufficient statistics given an observation,\n- :python:`pdf`, which is the density,\n- :python:`fisher_information`, which is the Fisher information matrix, and\n- :python:`entropy`, which is the Shannon entropy.\n\n\nEvery :python:`ExpectationParametrization` has a :python:`cross_entropy` method that has an\nefficient, numerically optimized custom JAX gradient.  This is possible because the gradient of the\ncross entropy is the difference of expectation parameters (when the expected carrier measure is\nzero).\n\nNumerical optimization\n----------------------\nBecause of the nature of the log-normalizer and carrier measure, some methods for some distributions\nrequire numerical optimization.  These are the conversion from expectation parameters to natural\nones, the entropy, and the cross entropy.\n\nDistributions\n=============\nEFAX supports the following distributions:\n\n- Bernoulli\n- beta\n- chi-square\n- complex normal\n- Dirichlet\n- exponential\n- gamma\n- geometric\n- isotropic normal (multivariate normal with isotropic variance)\n- logarithmic\n- multinomial\n- multivariate normal\n- multivariate unit normal (multivariate normal with isotropic unit variance)\n- negative binomial\n- normal\n- Poisson\n- von Mises-Fisher\n\nUsage\n=====\nBasic usage\n-----------\nA basic use of the two parametrizations:\n\n.. code:: python\n\n    from jax import numpy as jnp\n\n    from efax import BernoulliEP, BernoulliNP\n\n    # p is the expectation parameters of three Bernoulli distributions having probabilities 0.4, 0.5,\n    # and 0.6.\n    p = BernoulliEP(jnp.array([0.4, 0.5, 0.6]))\n\n    # q is the natural parameters of three Bernoulli distributions having log-odds 0, which is\n    # probability 0.5.\n    q = BernoulliNP(jnp.zeros(3))\n\n    print(p.cross_entropy(q))\n    # [0.6931472 0.6931472 0.6931472]\n\n    # q2 is natural parameters of Bernoulli distributions having a probability of 0.3.\n    p2 = BernoulliEP(0.3 * jnp.ones(3))\n    q2 = p2.to_nat()\n\n    print(p.cross_entropy(q2))\n    # [0.6955941  0.78032386 0.86505365]\n    # A Bernoulli distribution with probability 0.3 predicts a Bernoulli observation with probability\n    # 0.4 better than the other observations.\n\nOptimization\n------------\nUsing the cross entropy to iteratively optimize a prediction is simple:\n\n.. code:: python\n\n    from jax import grad, jit, lax\n    from jax import numpy as jnp\n\n    from efax import BernoulliEP, BernoulliNP\n\n\n    def cross_entropy_loss(p, q):\n        return p.cross_entropy(q)\n\n\n    gce = jit(grad(cross_entropy_loss, 1))\n\n\n    def body_fun(q):\n        return BernoulliNP(q.log_odds - gce(some_p, q).log_odds * 1e-4)\n\n\n    def cond_fun(q):\n        return jnp.sum(gce(some_p, q).log_odds ** 2) > 1e-7\n\n\n    # some_p are expectation parameters of a Bernoulli distribution corresponding\n    # to probability 0.4.\n    some_p = BernoulliEP(jnp.array(0.4))\n\n    # some_q are natural parameters of a Bernoulli distribution corresponding to\n    # log-odds 0, which is probability 0.5.\n    some_q = BernoulliNP(jnp.array(0.0))\n\n    # Optimize the predictive distribution iteratively.\n    print(lax.while_loop(cond_fun, body_fun, some_q))\n    # Outputs the natural parameters that correspond to 0.4.\n\n    # Compare with the true value.\n    print(some_p.to_nat())\n\nContribution guidelines\n=======================\n\nContributions are welcome!\n\nIf you want to add a new distribution, the steps are\n\n- Create an issue for the new distribution.\n\n- Solve for or research the equations needed to fill the blanks in the overview pdf, and put them in\n  the issue.  I'll add them to the pdf for you.\n\n- Implement the natural and expectation parametrizations.\n\n- Add the new distribution to the tests.\n\nImplementation should respect PEP8.\nThe tests can be run using :bash:`pytest .`\nThere are a few tools to clean and check the source:\n\n- :bash:`isort .`\n\n- :bash:`pylint efax`\n\n- :bash:`flake8 efax`\n\n- :bash:`mypy efax`\n",
    'author': 'Neil Girdhar',
    'author_email': 'mistersheik@gmail.com',
    'maintainer': None,
    'maintainer_email': None,
    'url': 'https://github.com/NeilGirdhar/efax',
    'packages': packages,
    'package_data': package_data,
    'install_requires': install_requires,
    'python_requires': '>=3.8,<4.0',
}


setup(**setup_kwargs)
