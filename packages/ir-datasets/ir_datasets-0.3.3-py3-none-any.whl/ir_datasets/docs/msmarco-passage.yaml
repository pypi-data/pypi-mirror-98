_:
  pretty_name: 'MSMARCO (passage)'
  desc: '
<p>
A passage ranking benchmark with a collection of 8.8 million passages and question queries. Most
relevance judgments are shallow (typically at most 1-2 per query), but the TREC Deep Learning track
adds deep judgments. Evaluation typically conducted using MRR@10.
</p>
<p>
Note that the original document source files for this collection contain a double-encoding error that
cause strange sequences like "Ã¥Â¬" and "Ã°ÂºÃ°". These are automatically corrrected (properly converting
previous examples to "å…¬" and "ðŸ‡ºðŸ‡¸").
</p>
<ul>
  <li>See also: <a class="ds-ref">msmarco-document</a></li>
  <li>Documents: Short passages (from web)</li>
  <li>Queries: Natural language questions (from query log)</li>
  <li><a href="https://microsoft.github.io/msmarco/#ranking">Leaderboard</a></li>
  <li><a href="https://arxiv.org/abs/1611.09268">Dataset Paper</a></li>
</ul>'
  bibtex: |
    @inproceedings{Bajaj2016MSMA,
      title={MS MARCO: A Human Generated MAchine Reading COmprehension Dataset},
      author={Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, Tong Wang},
      booktitle={InCoCo@NIPS},
      year={2016}
    }


dev:
  desc: '
<p>
Official dev set.
</p>
<p>
scoreddocs are the top 1000 results from BM25. These are used for the "re-ranking" setting. Note
that these are sub-sampled to about 1/8 of the total avaialable dev queries by the MSMARCO authors
for faster evaluation. The BM25 scores from scoreddocs are not available (all have a score of 0).
</p>
'

dev/small:
  desc: '
<p>
Official "small" version of the dev set, consisting of 6,980 queries (6.9% of the full dev set).
</p>
'


dev/judged:
  desc: '
<p>
Subset of <a class="ds-ref">msmarco-passage/dev</a> that only includes queries that have at least
one qrel.
</p>
'


eval:
  desc: '
<p>
Official eval set for submission to MS MARCO leaderboard. Relevance judgments are hidden.
</p>
<p>
scoreddocs are the top 1000 results from BM25. These are used for the "re-ranking" setting. Note
that these are sub-sampled to about 1/8 of the total avaialable eval queries by the MSMARCO authors
for faster evaluation. The BM25 scores from scoreddocs are not available (all have a score of 0).
</p>
'

eval/small:
  desc: '
<p>
Official "small" version of the eval set, consisting of 6,837 queries (6.8% of the full eval set).
</p>
'


train:
  desc: '
<p>
Official train set.
</p>
<p>
Not all queries have relevance judgments. Use <a class="ds-ref">msmarco-passage/train/judged</a> for
a filtered list that only includes documents that have at least one qrel.
</p>
<p>
scoreddocs are the top 1000 results from BM25. These are used for the "re-ranking" setting. Note
that these are sub-sampled to about 1/8 of the total avaialable train queries by the MSMARCO authors
for faster evaluation. The BM25 scores from scoreddocs are not available (all have a score of 0).
</p>
<p>
docpairs provides access to the "official" sequence for pairwise training.
</p>
'


train/judged:
  desc: '
<p>
Subset of <a class="ds-ref">msmarco-passage/train</a> that only includes queries that have at least
one qrel.
</p>
'


train/medical:
  desc: '
<p>
Subset of <a class="ds-ref">msmarco-passage/train</a> that only includes queries that have a layman
or expert medical term. Note that this includes about 20% false matches due to terms with multiple
senses.
</p>'
  bibtex: |
    @inproceedings{macavaney:emnlp2020-sledge,
      author = {MacAvaney, Sean and Cohan, Arman and Goharian, Nazli},
      title = {SLEDGE-Zero: A Zero-Shot Baseline for COVID-19 Literature Search},
      booktitle = {EMNLP},
      year = {2020}
    }


train/split200-train:
  desc: '
<p>
Subset of <a class="ds-ref">msmarco-passage/train</a> without 200 queries that are meant to be used
as a small validation set. From various works.
</p>
'


train/split200-valid:
  desc: '
<p>
Subset of <a class="ds-ref">msmarco-passage/train</a> with only 200 queries that are meant to be used
as a small validation set. From various works.
</p>
'


trec-dl-2019:
  desc: '
<p>
Queries from the TREC Deep Learning (DL) 2019 shared task, which were sampled from
<a class="ds-ref">msmarco-passage/eval</a>. A subset of these queries were judged by NIST assessors,
(filtered list available in <a class="ds-ref">msmarco-passage/trec-dl-2019/judged</a>).
</p>
<ul>
<li><a href="https://arxiv.org/pdf/2003.07820.pdf">Shared Task Paper</a></li>
</ul>
'
  bibtex: |
    @inproceedings{Craswell2020OverviewOT,
      title={Overview of the TREC 2019 deep learning track},
      author={Nick Craswell and Bhaskar Mitra and Emine Yilmaz and Daniel Campos and Ellen Voorhees},
      booktitle={TREC 2019},
      year={2019}
    }


trec-dl-2019/judged:
  desc: '
<p>
Subset of <a class="ds-ref">msmarco-passage/trec-dl-2019</a>, only including queries with qrels.
</p>
'


trec-dl-2020:
  desc: '
<p>
Queries from the TREC Deep Learning (DL) 2020 shared task, which were sampled from
<a class="ds-ref">msmarco-passage/eval</a>. A subset of these queries were judged by NIST assessors,
(filtered list available in <a class="ds-ref">msmarco-passage/trec-dl-2020/judged</a>).
</p>
<ul>
<li><a href="https://arxiv.org/pdf/2102.07662.pdf">Shared Task Paper</a></li>
</ul>
'
  bibtex: |
    @inproceedings{Craswell2021OverviewOT,
      title={Overview of the TREC 2020 deep learning track},
      author={Nick Craswell and Bhaskar Mitra and Emine Yilmaz and Daniel Campos},
      booktitle={TREC},
      year={2020}
    }

trec-dl-2020/judged:
  desc: '
<p>
Subset of <a class="ds-ref">msmarco-passage/trec-dl-2020</a>, only including queries with qrels.
</p>
'
