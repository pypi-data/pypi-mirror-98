{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import json\n",
    "import boto3\n",
    "import io\n",
    "import s3fs\n",
    "from smart_open import open as s_open\n",
    "\n",
    "import xlsxwriter\n",
    "from itertools import product\n",
    "import featuretools as ft\n",
    "\n",
    "import re\n",
    "from featuretools.primitives import make_agg_primitive,make_trans_primitive\n",
    "from featuretools.variable_types import DateOfBirth,Text,Numeric,Categorical,Boolean,Index,DatetimeTimeIndex,Datetime,TimeIndex,Discrete,Ordinal\n",
    "from scipy.stats import mode\n",
    "from featuretools.primitives import TimeSinceLast,TimeSinceFirst,AvgTimeBetween,TimeSince,TimeSincePrevious\n",
    "from v_ft.custom_primitives import *\n",
    "from v_ft import time_unit as tu\n",
    "import time\n",
    "from pandas.api.types import is_string_dtype\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name ='automating-travis'\n",
    "folder_name = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COL_PLACEHOLDER = [\"Don't have\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. prepare config for feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def pd_read(file_name,src='s3',sample=None,used_cols=None):\n",
    "\n",
    "    csvFile = os.path.join('s3://',bucket_name,folder_name,file_name)\n",
    "    table=None\n",
    "    if csvFile.endswith('.csv'):\n",
    "        table = pd.read_csv(csvFile, nrows=sample,usecols=used_cols)\n",
    "    if csvFile.endswith('.xlsx'):\n",
    "        s3_c = boto3.client('s3')\n",
    "        file_path = os.path.join(folder_name,file_name)\n",
    "        obj = s3_c.get_object(Bucket=bucket_name, Key=file_path)\n",
    "        data = obj['Body'].read()\n",
    "        table = pd.read_excel(io.BytesIO(data),nrows=sample)\n",
    "            \n",
    "    if csvFile.endswith('.parquet'):\n",
    "        table = pd.read_parquet(csvFile,engine='pyarrow',columns=used_cols)\n",
    "        if sample:\n",
    "            table = table.head(sample)\n",
    "    return table\n",
    "\n",
    "def create_bin(df,col):\n",
    "    bins = []\n",
    "\n",
    "    for val in df[col]:\n",
    "        if \"<=\" in val:\n",
    "            val = int(val.strip(\"<=\"))\n",
    "        elif \">=\" in val:\n",
    "            continue\n",
    "            #val = int(val.strip(\">=\"))\n",
    "        else:\n",
    "            val = int(val.split(\"-\")[-1])\n",
    "        bins.append(val)\n",
    "    return bins\n",
    "\n",
    "def my_cut(col,x, bins,\n",
    "            lower_infinite=True, upper_infinite=True,\n",
    "            **kwargs):\n",
    "    r\"\"\"Wrapper around pandas cut() to create infinite lower/upper bounds with proper labeling.\n",
    "\n",
    "    Takes all the same arguments as pandas cut(), plus two more.\n",
    "\n",
    "    Args :\n",
    "        lower_infinite (bool, optional) : set whether the lower bound is infinite\n",
    "            Default is True. If true, and your first bin element is something like 20, the\n",
    "            first bin label will be '<= 20' (depending on other cut() parameters)\n",
    "        upper_infinite (bool, optional) : set whether the upper bound is infinite\n",
    "            Default is True. If true, and your last bin element is something like 20, the\n",
    "            first bin label will be '> 20' (depending on other cut() parameters)\n",
    "        **kwargs : any standard pandas cut() labeled parameters\n",
    "\n",
    "    Returns :\n",
    "        out : same as pandas cut() return value\n",
    "        bins : same as pandas cut() return value\n",
    "    \"\"\"\n",
    "\n",
    "    # Quick passthru if no infinite bounds\n",
    "    if not lower_infinite and not upper_infinite:\n",
    "        return pd.cut(x, bins, **kwargs)\n",
    "\n",
    "    # Setup\n",
    "    num_labels      = len(bins) - 1\n",
    "    include_lowest  = kwargs.get(\"include_lowest\", False)\n",
    "    right           = kwargs.get(\"right\", True)\n",
    "\n",
    "    # Prepend/Append infinities where indiciated\n",
    "    bins_final = bins.copy()\n",
    "    if upper_infinite:\n",
    "        bins_final.insert(len(bins),float(\"inf\"))\n",
    "        num_labels += 1\n",
    "    if lower_infinite:\n",
    "        bins_final.insert(0,float(\"-inf\"))\n",
    "        num_labels += 1\n",
    "\n",
    "    # Decide all boundary symbols based on traditional cut() parameters\n",
    "    symbol_lower  = \"<=\" if include_lowest and right else \"<\"\n",
    "    left_bracket  = \"(\" if right else \"[\"\n",
    "    right_bracket = \"]\" if right else \")\"\n",
    "    symbol_upper  = \">\" if right else \">=\"\n",
    "\n",
    "    # Inner function reused in multiple clauses for labeling\n",
    "    def make_label(i, lb=left_bracket, rb=right_bracket):\n",
    "        return \"{0}{1}, {2}{3}\".format(lb, bins_final[i], bins_final[i+1], rb)\n",
    "\n",
    "    # Create custom labels\n",
    "    labels=[]\n",
    "    for i in range(0,num_labels):\n",
    "        new_label = None\n",
    "\n",
    "        if i == 0:\n",
    "            if lower_infinite:\n",
    "                new_label = \"{0} {1}\".format(symbol_lower, bins_final[i+1])\n",
    "            elif include_lowest:\n",
    "                new_label = make_label(i, lb=\"[\")\n",
    "            else:\n",
    "                new_label = make_label(i)\n",
    "        elif upper_infinite and i == (num_labels - 1):\n",
    "            new_label = \"{0} {1}\".format(symbol_upper, bins_final[i])\n",
    "        else:\n",
    "            new_label = make_label(i)\n",
    "\n",
    "        labels.append(new_label)\n",
    "    ref_map[col]=labels\n",
    "    # Pass thru to pandas cut()\n",
    "    return pd.cut(x, bins_final, labels=labels, **kwargs)\n",
    "\n",
    "\n",
    "def single_itr(fts:list,df):\n",
    "    # fts like ['age','sp']\n",
    "    itr_cols=[]\n",
    "    itr_vals=[]\n",
    "    for ft in fts:\n",
    "        itr_cols.append(ft)\n",
    "        if ft in ref_map:\n",
    "            itr_vals.append(ref_map[ft])\n",
    "        else:\n",
    "            itr_vals.append(df[ft].unique().tolist())\n",
    "\n",
    "    return itr_cols,itr_vals\n",
    "\n",
    "def prod_itr(fts:list,df):\n",
    "    # fts is like ['age|covergage','age|SP']\n",
    "    # df should be the data in config['entities']\n",
    "    # ref_map below is like {'SP':['Y','N']}\n",
    "    itr_cols=[]\n",
    "    itr_vals=[]\n",
    "    for ft in fts:\n",
    "        ft_used = ft.split('|')\n",
    "        new_ft = '_'.join(ft_used)\n",
    "        itr_cols.append(new_ft)\n",
    "        if new_ft not in df:\n",
    "            df[new_ft]=df[ft_used[0]].astype(str)+\"_\"+df[ft_used[1]].astype(str) # generate new column concat with two others\n",
    "        unique_vals=[ref_map[f] if f in ref_map else df[f].unique().tolist() for f in ft_used] #get unique column values, check if in the reference table list\n",
    "        print(unique_vals)\n",
    "        itr_val = [x[0]+'_'+x[1] for x in list(product(unique_vals[0],unique_vals[1]))] #get the product values for new feature values\n",
    "        itr_vals.append(itr_val)\n",
    "    return itr_cols,itr_vals\n",
    "def prepare_config(config):#path,config_name):\n",
    "    #with s_open(os.path.join(path,'{}.pickle'.format(config_name)), 'rb') as handle:\n",
    "    #    config = pickle.load(handle)\n",
    "\n",
    "\n",
    "    #get data\n",
    "    for i,entity in enumerate(config['entities']):\n",
    "        data_name = config['all_data'][entity['entity']]\n",
    "        print('data to read:',data_name)\n",
    "        data = pd_read(data_name,src='s3',sample=1000)\n",
    "        #config[entity['entity']]=eval(entity['entity'])\n",
    "\n",
    "\n",
    "    #map reference if have and interesting values\n",
    "\n",
    "\n",
    "    # deal with reference first\n",
    "    for pvt in config['pivotings']:\n",
    "        if pvt['reference']!=COL_PLACEHOLDER[0]: #only accept for reference for one column; which means we only have 1 single or 1 prod feature for this time, i.e. len(fts)==0\n",
    "            # get data from entities\n",
    "            df_name = config['all_data'][pvt['entity']]\n",
    "            df = config[df_name]\n",
    "            # get features\n",
    "            if 'column2' in pvt:\n",
    "                print('has prod')\n",
    "                fts = pvt['column1'][0]+'_'+pvt['column2'][0] #get column name like [age,coverage]\n",
    "                df[fts]=df[pvt['column1'][0]].astype(str)+\"_\"+df[pvt['column2'][0]].astype(str) #get new column\n",
    "            else:\n",
    "                fts = pvt['column1'][0]\n",
    "            print('fts',fts)\n",
    "            grp = pd_read(pvt['reference'])\n",
    "            ref_type = pvt['ref_type']\n",
    "            if ref_type=='range':\n",
    "                #df_used = config['datas'][data_id]\n",
    "                fe = grp.columns[0]\n",
    "                bins = create_bin(grp,fe)\n",
    "                print(bins)\n",
    "                df[fts]=my_cut(fe,df[fts],bins,right=True,include_lowest=True)\n",
    "                #reference.append(pvt\n",
    "            else:\n",
    "                maps = pd.Series(grp.iloc[:,1].values,index=grp.iloc[:,0]).rename(fts).to_dict()\n",
    "                df.map(maps)\n",
    "            \n",
    "            # get interesting values\n",
    "            if pvt['kind']=='two':\n",
    "                itr_cols,itr_vals =prod_itr([fts],df)\n",
    "                pvt['columns']=itr_cols\n",
    "                pvt['values']=itr_vals\n",
    "            else:\n",
    "                itr_cols,itr_vals =single_itr([fts],df)\n",
    "                pvt['columns']=itr_cols\n",
    "                pvt['values']=itr_vals\n",
    "            \n",
    "    # get intereting values for others with no reference\n",
    "    #reference={}\n",
    "    for pvt in config['pivotings']:\n",
    "        if pvt['reference']==COL_PLACEHOLDER[0]: #only accept for reference for one column; which means we only have 1 single or 1 prod feature for this time, i.e. len(fts)==0\n",
    "            df_name = config['all_data'][pvt['entity']]\n",
    "            #get data\n",
    "            df = config[df_name]\n",
    "            #get pivot feature list and get interesting values\n",
    "            if pvt['kind']=='two':\n",
    "                fts = [x[0]+'|'+x[1] for x in list(product(pvt['column1'],pvt['column2']))] #fts is like ['age|covergage','age|SP']\n",
    "                print(fts)\n",
    "                itr_cols,itr_vals =prod_itr(fts,df)\n",
    "                df = prod_itr(fts,df)\n",
    "                pvt['columns']=itr_cols\n",
    "                pvt['values']=itr_vals\n",
    "            \n",
    "            else:\n",
    "\n",
    "                fts = pvt['column1'].split('|') #like ['age','sp']\n",
    "                print('iam in single',fts)\n",
    "                itr_cols,itr_vals =single_itr(fts,df)\n",
    "                pvt['columns']=itr_cols\n",
    "                pvt['values']=itr_vals\n",
    "                print('in single',(itr_cols,itr_vals))\n",
    "            \n",
    "    print('after itr, config is',config['pivotings'])\n",
    "\n",
    "    return config\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_bin(df,col):\n",
    "    bins = []\n",
    "\n",
    "    for val in df[col]:\n",
    "        if \"<=\" in val:\n",
    "            val = int(val.strip(\"<=\"))\n",
    "        elif \">=\" in val:\n",
    "            continue\n",
    "            #val = int(val.strip(\">=\"))\n",
    "        else:\n",
    "            val = int(val.split(\"-\")[-1])\n",
    "        bins.append(val)\n",
    "    return bins\n",
    "\n",
    "def my_cut(col,x, bins,\n",
    "            lower_infinite=True, upper_infinite=True,\n",
    "            **kwargs):\n",
    "    r\"\"\"Wrapper around pandas cut() to create infinite lower/upper bounds with proper labeling.\n",
    "\n",
    "    Takes all the same arguments as pandas cut(), plus two more.\n",
    "\n",
    "    Args :\n",
    "        lower_infinite (bool, optional) : set whether the lower bound is infinite\n",
    "            Default is True. If true, and your first bin element is something like 20, the\n",
    "            first bin label will be '<= 20' (depending on other cut() parameters)\n",
    "        upper_infinite (bool, optional) : set whether the upper bound is infinite\n",
    "            Default is True. If true, and your last bin element is something like 20, the\n",
    "            first bin label will be '> 20' (depending on other cut() parameters)\n",
    "        **kwargs : any standard pandas cut() labeled parameters\n",
    "\n",
    "    Returns :\n",
    "        out : same as pandas cut() return value\n",
    "        bins : same as pandas cut() return value\n",
    "    \"\"\"\n",
    "\n",
    "    # Quick passthru if no infinite bounds\n",
    "    if not lower_infinite and not upper_infinite:\n",
    "        return pd.cut(x, bins, **kwargs)\n",
    "\n",
    "    # Setup\n",
    "    num_labels      = len(bins) - 1\n",
    "    include_lowest  = kwargs.get(\"include_lowest\", False)\n",
    "    right           = kwargs.get(\"right\", True)\n",
    "\n",
    "    # Prepend/Append infinities where indiciated\n",
    "    bins_final = bins.copy()\n",
    "    if upper_infinite:\n",
    "        bins_final.insert(len(bins),float(\"inf\"))\n",
    "        num_labels += 1\n",
    "    if lower_infinite:\n",
    "        bins_final.insert(0,float(\"-inf\"))\n",
    "        num_labels += 1\n",
    "\n",
    "    # Decide all boundary symbols based on traditional cut() parameters\n",
    "    symbol_lower  = \"<=\" if include_lowest and right else \"<\"\n",
    "    left_bracket  = \"(\" if right else \"[\"\n",
    "    right_bracket = \"]\" if right else \")\"\n",
    "    symbol_upper  = \">\" if right else \">=\"\n",
    "\n",
    "    # Inner function reused in multiple clauses for labeling\n",
    "    def make_label(i, lb=left_bracket, rb=right_bracket):\n",
    "        return \"{0}{1}, {2}{3}\".format(lb, bins_final[i], bins_final[i+1], rb)\n",
    "\n",
    "    # Create custom labels\n",
    "    labels=[]\n",
    "    for i in range(0,num_labels):\n",
    "        new_label = None\n",
    "\n",
    "        if i == 0:\n",
    "            if lower_infinite:\n",
    "                new_label = \"{0} {1}\".format(symbol_lower, bins_final[i+1])\n",
    "            elif include_lowest:\n",
    "                new_label = make_label(i, lb=\"[\")\n",
    "            else:\n",
    "                new_label = make_label(i)\n",
    "        elif upper_infinite and i == (num_labels - 1):\n",
    "            new_label = \"{0} {1}\".format(symbol_upper, bins_final[i])\n",
    "        else:\n",
    "            new_label = make_label(i)\n",
    "\n",
    "        labels.append(new_label)\n",
    "    ref_map[col]=labels\n",
    "    # Pass thru to pandas cut()\n",
    "    return pd.cut(x, bins_final, labels=labels, **kwargs)\n",
    "\n",
    "\n",
    "def single_itr(fts:list,df):\n",
    "    # fts like ['age','sp']\n",
    "    itr_cols=[]\n",
    "    itr_vals=[]\n",
    "    for ft in fts:\n",
    "        itr_cols.append(ft)\n",
    "        if ft in ref_map:\n",
    "            itr_vals.append(ref_map[ft])\n",
    "        else:\n",
    "            itr_vals.append(df[ft].unique().tolist())\n",
    "\n",
    "    return itr_cols,itr_vals\n",
    "\n",
    "def prod_itr(fts:list,df):\n",
    "    # fts is like ['age|covergage','age|SP']\n",
    "    # df should be the data in config['entities']\n",
    "    # ref_map below is like {'SP':['Y','N']}\n",
    "    itr_cols=[]\n",
    "    itr_vals=[]\n",
    "    for ft in fts:\n",
    "        ft_used = ft.split('|')\n",
    "        new_ft = '_'.join(ft_used)\n",
    "        itr_cols.append(new_ft)\n",
    "        if new_ft not in df:\n",
    "            df[new_ft]=df[ft_used[0]].astype(str)+\"_\"+df[ft_used[1]].astype(str) # generate new column concat with two others\n",
    "        unique_vals=[ref_map[f] if f in ref_map else df[f].unique().tolist() for f in ft_used] #get unique column values, check if in the reference table list\n",
    "        print(unique_vals)\n",
    "        itr_val = [x[0]+'_'+x[1] for x in list(product(unique_vals[0],unique_vals[1]))] #get the product values for new feature values\n",
    "        itr_vals.append(itr_val)\n",
    "    return itr_cols,itr_vals\n",
    "\n",
    "\n",
    "def prepare_config(config,bucket_name,folder_name):#path,config_name):\n",
    "    #with s_open(os.path.join(path,'{}.pickle'.format(config_name)), 'rb') as handle:\n",
    "    #    config = pickle.load(handle)\n",
    "\n",
    "\n",
    "    #get data\n",
    "    for i,entity in enumerate(config['entities']):\n",
    "        data_name = config['all_data'][entity['entity']]\n",
    "        print('data to read:',data_name)\n",
    "        data = pd_read(data_name,src='s3',sample=1000)\n",
    "        config[data_name]=data\n",
    "\n",
    "    #map reference if have and interesting values\n",
    "\n",
    "\n",
    "    # deal with reference first\n",
    "    for pvt in config['pivotings']:\n",
    "        if pvt['reference']!=COL_PLACEHOLDER[0]: #only accept for reference for one column; which means we only have 1 single or 1 prod feature for this time, i.e. len(fts)==0\n",
    "            # get data from entities\n",
    "            df_name = config['all_data'][pvt['entity']]\n",
    "            df = config[df_name]\n",
    "            # get features\n",
    "            if 'column2' in pvt:\n",
    "                print('has prod')\n",
    "                fts = pvt['column1']+'|'+pvt['column2'] #get column name like 'age|coverage'\n",
    "                df[fts]=df[pvt['column1']].astype(str)+\"_\"+df[pvt['column2']].astype(str) #get new column\n",
    "            else:\n",
    "                print('in refereence not prod with pvt',pvt)\n",
    "                fts = pvt['column1']\n",
    "            print('fts',fts)\n",
    "            grp = pd_read(config['all_data'][pvt['reference']],bucket_name=bucket_name,pre=folder_name,src='s3')\n",
    "            ref_type = pvt['ref_type']\n",
    "            if ref_type=='range':\n",
    "                #df_used = config['datas'][data_id]\n",
    "                fe = grp.columns[0]\n",
    "                bins = create_bin(grp,fe)\n",
    "                print(bins)\n",
    "                df[fts]=my_cut(fe,df[fts],bins,right=True,include_lowest=True)\n",
    "                #reference.append(pvt\n",
    "            else:\n",
    "                maps = pd.Series(grp.iloc[:,1].values,index=grp.iloc[:,0]).rename(fts).to_dict()\n",
    "                df.map(maps)\n",
    "\n",
    "            # get interesting values\n",
    "            if pvt['kind']=='two':\n",
    "                itr_cols,itr_vals =prod_itr([fts],df)\n",
    "                pvt['columns']=itr_cols\n",
    "                pvt['values']=itr_vals\n",
    "            else:\n",
    "                itr_cols,itr_vals =single_itr([fts],df)\n",
    "                pvt['columns']=itr_cols\n",
    "                pvt['values']=itr_vals\n",
    "    # get intereting values for others with no reference\n",
    "    #reference={}\n",
    "    for pvt in config['pivotings']:\n",
    "        if pvt['reference']==COL_PLACEHOLDER[0]: #only accept for reference for one column; which means we only have 1 single or 1 prod feature for this time, i.e. len(fts)==0\n",
    "            df_name = config['all_data'][pvt['entity']]\n",
    "            #get data\n",
    "            df = config[df_name]\n",
    "            #get pivot feature list and get interesting values\n",
    "            if pvt['kind']=='two':\n",
    "                fts = [x[0]+'|'+x[1] for x in list(product(pvt['column1'].split('|'),pvt['column2'].split('|')))] #fts is like ['age|covergage','age|SP']\n",
    "                itr_cols,itr_vals =prod_itr(fts,df)\n",
    "                pvt['columns']=itr_cols\n",
    "                pvt['values']=itr_vals\n",
    "            else:\n",
    "\n",
    "                fts = pvt['column1'].split('|') #like ['age','sp']\n",
    "                print('iam in single',fts)\n",
    "                itr_cols,itr_vals =single_itr(fts,df)\n",
    "                pvt['columns']=itr_cols\n",
    "                pvt['values']=itr_vals\n",
    "                print('in single',(itr_cols,itr_vals))\n",
    "    print('after itr, config is',config['pivotings'])\n",
    "\n",
    "    return config\n",
    "\n",
    "def get_features(bucket_name,folder_name,config_name,schema_name):\n",
    "    #config = prepare_config(path,config_name)\n",
    "    path = os.path.join('s3://',bucket_name,folder_name)\n",
    "    with s_open(os.path.join(path,'{}.pickle'.format(config_name)), 'rb') as handle:\n",
    "        config = pickle.load(handle)\n",
    "    config_prd = prepare_config(config,bucket_name,folder_name)\n",
    "    ent = ft_agg(config_prd)\n",
    "    en_set,cutoff = ent.get_entityset()\n",
    "    agg_used,tfm_used,whr_pmts = ent.get_prmts()\n",
    "    features = ent.run_dfs(es=en_set,cutoff=cutoff,featureonly=True)\n",
    "    print('save to:',os.path.join(path,\"{}.json\".format(schema_name)))\n",
    "    ft.save_features(features, os.path.join(path,\"{}.json\".format(schema_name)))\n",
    "\n",
    "    target_idx_columns = ent.target_idx_columns\n",
    "    features_out = target_idx_columns+[f.get_name() for f in features]\n",
    "    return features_out#,features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. feature engineerin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # featuretools class\n",
    "\n",
    "\n",
    "class ft_agg(object):\n",
    "    def __init__(self,config):\n",
    "        self.config = config\n",
    "        self.entities = config['entities']\n",
    "        self.relations = config['relations']\n",
    "        self.pivotings = config['pivotings']\n",
    "        #self.datasets=[[config['names'][i],config['datas'][i],config['keys'][i]] for i in range(len(config['names']))]\n",
    "        #self.frn_keys = config['frn_keys']\n",
    "        #self.relations = config['relations']\n",
    "\n",
    "        #self.time_idx = config['time_index']\n",
    "\n",
    "        self.agg_kpt = config['pmt_agg_kpt']\n",
    "        self.tfm_kpt = config['pmt_tfm_kpt']\n",
    "        #self.n_sample = config['samples']\n",
    "        #self.ign_var_all = config['ignore_vars']\n",
    "        #self.k_n_drp=config['keys_not_drp']\n",
    "        self.time_unit = 'year'#config['t_unit'] #year,month,day, with year as default\n",
    "\n",
    "        ### get intr config###\n",
    "        #self.if_intr = config['if_intr_value']\n",
    "        #self.if_prod_itr = config['if_prod_itr']\n",
    "        if self.pivotings:\n",
    "            self.whr_pmt = config['where_primitives']\n",
    "            #self.itr_fts = config['itr_fts']\n",
    "            #self.itr_vals = config['itr_vals']\n",
    "        #if self.if_prod_itr:\n",
    "        #    self.prod_itr_fts=config['prod_itr_fts']\n",
    "        #    self.prod_itr_vals =config['prod_itr_vals']\n",
    "\n",
    "        else:# (not self.if_intr) and (not self.if_prod_itr):\n",
    "            self.whr_pmt = []\n",
    "            #self.itr_val = None\n",
    "\n",
    "\n",
    "        # cutoff config\n",
    "        self.if_cut = False #config['has_cutoff']\n",
    "\n",
    "        self.cutoff = None # config['cutoff_date']\n",
    "        # check if use cutoff dataset\n",
    "        #if config['use_cutoff_df']:\n",
    "        #    self.cutoff = config['cut_df'].get_dataframe()\n",
    "            #config['cut_df'] = None\n",
    "        #    #config['cut_df_idx']=None\n",
    "        #    self.cutoff_idx = config['cut_df_idx']\n",
    "\n",
    "        # primitive options\n",
    "        self.options = config['primitive_options']\n",
    "        self.ig_vars = {}\n",
    "\n",
    "    def get_prmts(self):\n",
    "        # 1 define customized primitives\n",
    "        #### already done by import v_ft\n",
    "\n",
    "        # 2 change the time units of time-related primitives\n",
    "\n",
    "        t = tu.time_unit(self.time_unit) #from time_unit in v_ft\n",
    "        time_since_last = t.time_since_last\n",
    "        time_since_first = t.time_since_first\n",
    "        avg_time_between = t.avg_time_between\n",
    "        time_since = t.time_since\n",
    "        time_since_previous = t.time_since_previous\n",
    "        #avg_age = AvgAge(time=self.cutoff)\n",
    "        # 3 get used primitives'count,if_exist,time_since_first,time_since_last'\n",
    "        whr_dic = {'count':'count','if_exist':enco,'time_since_first':time_since_first,'time_since_last':time_since_last}\n",
    "        whr_pmts = [whr_dic[n] for n in self.whr_pmt]\n",
    "        time_agg = {'if_exist':enco,'max_age':max_age,'min_age':min_age,'max_bol':max_bol,'sum_bol':sum_bol,'time_since_first':time_since_first,'time_since_last':time_since_last,'avg_time_between':avg_time_between,'min_time':min_d,'max_time':max_d}\n",
    "        time_tfm = {'time_since':time_since,'time_since_previous':time_since_previous,'seasons':seasons,'partofday':PartDay,'week_day':week_day}\n",
    "        #all_pmt = ft.list_primitives()\n",
    "        #agg_pmt = ['sum','percent_true','num_unique','any','min','entropy','trend',\\\n",
    "        #           'first','mean','skew','count','num_true','all','max','last','mode','std','median']\n",
    "        #tfm_pmt = ['hour','latitude','minute','month','weekday','is_null','cum_min','cum_count',\\\n",
    "        #         'week','longitude','second','time_since','day','percentile','year',\\\n",
    "        #         'time_since_previous','cum_max','is_weekend','haversine']\n",
    "        #agg_custm = [min_d,max_d]#,time_since_last,time_since_first,avg_time_between]\n",
    "        #tfm_custm = [time_since,time_since_previous,season]\n",
    "        #print('tfm_kpt is:, ',self.tfm_kpt)\n",
    "        #print('tfm jud: ',self.tfm_kpt[0]=='')\n",
    "        agg_used = list(set([time_agg[p] if p in time_agg else p for p in self.agg_kpt]  + whr_pmts))\n",
    "        print('agg_used',agg_used)\n",
    "        print('agg_not_whr',[time_agg[p] if p in time_agg else p for p in self.agg_kpt])\n",
    "        print('agg wher',whr_pmts)\n",
    "        if self.tfm_kpt:\n",
    "            tfm_used = list(set([time_tfm[p] if p in time_tfm else p for p in self.tfm_kpt]))\n",
    "        else:\n",
    "            tfm_used = []\n",
    "        #self.config['agg_used'] = agg_used\n",
    "        #self.config['tfm_used'] = tfm_used\n",
    "        return agg_used,tfm_used,whr_pmts\n",
    "\n",
    "\n",
    "    def get_info(self,df_name,info_name):\n",
    "            for entity in self.entities:\n",
    "                if entity['entity']==df_name:\n",
    "                    return entity[info_name]\n",
    "\n",
    "    def add_r(self,es,one_df,one_col,many_df,many_col):\n",
    "        es = es.add_relationship(ft.Relationship(es[one_df][one_col],es[many_df][many_col]))\n",
    "        return es\n",
    "\n",
    "    def gen_idx(self,df,cols_k=None,name_key=None,keep_cols=[]): #no need to drop since index only has count method (and for those concat keys, since the single column is not index, so count won't be used)\n",
    "        # concat keys and drop related cols but keep those also used to do calculation (like c_loss_typ in IP)\n",
    "        print('col are',df.columns)\n",
    "        print('key is',cols_k)\n",
    "        print('name key is',name_key)\n",
    "        #col_drp = []\n",
    "        if not name_key in df.columns and name_key and cols_k:\n",
    "            print('generate key')\n",
    "            df[name_key]=df[cols_k].apply(lambda row: '_'.join(row.values.astype(str)), axis=1)\n",
    "            #col_drp+=cols_k\n",
    "        #print(col_drp)\n",
    "        #col_drp = list(set(col_drp)-set(keep_cols))\n",
    "       # print('col to drop',col_drp)\n",
    "        #print('cols have',df.columns)\n",
    "        #df = df.drop(columns=list(set(col_drp)))\n",
    "        #print('after drop',df)\n",
    "        return df\n",
    "\n",
    "    def dup_chk(self,datasets):\n",
    "        # use the key subset\n",
    "        for data in datasets:\n",
    "            data[2].drop_duplicates(subset=data[1], keep='first', inplace=True)\n",
    "\n",
    "    def get_entityset(self):\n",
    "\n",
    "        ###############################################\n",
    "        #### generate keys and frn keysfor each table #\n",
    "        ###############################################\n",
    "\n",
    "        for entity in self.entities:\n",
    "            # deal with time_idx\n",
    "            if entity['time_idx']==COL_PLACEHOLDER[0]:\n",
    "                entity['time_idx']=None\n",
    "            # get target alias\n",
    "            if entity['if_target']=='yes':\n",
    "                self.target=entity['alias']\n",
    "                self.target_idx_columns=entity['idx'].split('|') #used to recover concat index features (since index column will be in the output features' index)\n",
    "                self.target_key = entity['alias']+'_key'\n",
    "            if not '|' in entity['idx']:\n",
    "                entity['key']=entity['idx']\n",
    "                continue\n",
    "            cols_key = entity['idx'].split('|') #key columns\n",
    "            #self.ig_vars[entity['alias']]=cols_key #add the col that consitute the index and will be ignore during dfs\n",
    "            data_cur = self.config[entity['entity']] #data\n",
    "            idx_key = entity['alias']+'_key' #new key name\n",
    "            entity['key']=idx_key #change name\n",
    "            data_cur = self.gen_idx(data_cur,cols_key,idx_key) # no need to save back since it is mutable varaible\n",
    "\n",
    "\n",
    "        for relation in self.relations:\n",
    "            # get one and many alias\n",
    "            one_alias = self.get_info(relation['one'],'alias')\n",
    "            many_alias = self.get_info(relation['many'],'alias')\n",
    "            # get key name\n",
    "            frn_idx_key = many_alias+'_frn_'+one_alias\n",
    "            #get many side data\n",
    "            frn_data_cur = self.config[relation['many']]\n",
    "            #get frn key columns\n",
    "            frn_cols_key = relation['foreign_key'].split('|')\n",
    "            #change parameters in reation\n",
    "            relation['foreign_key'] = frn_idx_key\n",
    "            relation['one']=one_alias\n",
    "            relation['many']=many_alias\n",
    "            relation['one_key']=relation['one']+'_key'\n",
    "            frn_data_cur = self.gen_idx(frn_data_cur,frn_cols_key,frn_idx_key) # no need to save back since it is mutable varaible\n",
    "\n",
    "\n",
    "        es = ft.EntitySet() #id='itli'\n",
    "        # add entity\n",
    "        ## check duplicate keys (should be avoided in the future)\n",
    "        #self.dup_chk(new_datasets)\n",
    "\n",
    "\n",
    "        ##################################\n",
    "        # get cutoff date for processing#\n",
    "        ##################################\n",
    "        if self.if_cut:\n",
    "            df_4_cutoff = new_datasets[-1][2]\n",
    "            if not self.cutoff:\n",
    "                self.cutoff = 'cutoff'\n",
    "                df_4_cutoff[self.cutoff] =  pd.to_datetime('today')\n",
    "\n",
    "            if self.config['use_cutoff_df']:\n",
    "                print('i am in use cutoff dataset')\n",
    "                cut_off_df=self.gen_idx(self.cutoff,self.cutoff_idx,new_datasets[-1][1])\n",
    "\n",
    "            else:\n",
    "                if is_numeric_dtype(df_4_cutoff[self.cutoff]) or is_string_dtype(df_4_cutoff[self.cutoff]):\n",
    "                    df_4_cutoff[self.cutoff]=pd.to_datetime(df_4_cutoff[self.cutoff],errors='coerce')\n",
    "                cut_off_df = df_4_cutoff[[new_datasets[-1][1],self.cutoff]].drop_duplicates()\n",
    "\n",
    "               #make cotoff time at target entity level and no duplicates\n",
    "        else:\n",
    "            cut_off_df=None\n",
    "\n",
    "\n",
    "        ##################################\n",
    "          # add entities and relations\n",
    "          #  frn names is list right now, so new_datasets[i][3][0]\n",
    "          # #\n",
    "        ##################################\n",
    "        '''\n",
    "        itr_data = [new_datasets[i][2].copy() for i in range(len(new_datasets))]\n",
    "        if self.config['process']=='dev' or self.config['prod_sample']:\n",
    "            print('get sample')\n",
    "            n_sample =int(config['prod_sample'] or 1000)\n",
    "            if self.if_cut:\n",
    "                #print('check cut0')\n",
    "                cut_off_df = cut_off_df.iloc[:n_sample,:]\n",
    "                #print('check cut',cut_off_df.head())\n",
    "\n",
    "            print('type is',type(new_datasets[-1][2]))\n",
    "            new_datasets[-1][2] = new_datasets[-1][2].iloc[:n_sample,:]\n",
    "\n",
    "        '''\n",
    "        for entity in self.entities:\n",
    "            idx_used = entity['key']\n",
    "            if idx_used:\n",
    "                mk_idx=False\n",
    "            else:\n",
    "                idx_used=entity['alias']+'_key'\n",
    "                mk_idx=True\n",
    "\n",
    "            es.entity_from_dataframe(entity_id = entity['alias'],\n",
    "                                     dataframe=self.config[entity['entity']],\n",
    "                                     make_index=mk_idx,\n",
    "                                     index=idx_used,\n",
    "                                     time_index=entity['time_idx'])\n",
    "        '''\n",
    "        for i,data in enumerate(new_datasets):\n",
    "            print('data and time_idx',(data[0],self.time_idx[i]))\n",
    "            es.entity_from_dataframe(entity_id = data[0],\n",
    "                                     dataframe=data[2],\n",
    "                                     index=data[1],\n",
    "                                     time_index=self.time_idx[i])#,\n",
    "                                    #variable_types=self.vtype[i])\n",
    "        '''\n",
    "            #ents.append([data[0],])\n",
    "        # add relations (one to many: one_df, one_key, many_df,many_frn)\n",
    "        for relation in self.relations:\n",
    "            es = self.add_r(es,relation['one'],relation['one_key'],relation['many'],relation['foreign_key'])\n",
    "        '''\n",
    "        for r in self.relations:\n",
    "            if isinstance(r[0],int):\n",
    "                i,j = r\n",
    "                print('rela columns',es[new_datasets[i][0]].df.columns)\n",
    "                print('frn_key',new_datasets[i][3])\n",
    "                es = self.add_r(es,new_datasets[j][0],new_datasets[j][1],new_datasets[i][0],new_datasets[i][3][0])\n",
    "            else:\n",
    "                for n,(i,j) in enumerate(r):\n",
    "                    es = self.add_r(es,new_datasets[j][0],new_datasets[j][1],new_datasets[i][0],new_datasets[i][3][n])\n",
    "        '''\n",
    "            #es = self.add_r(es,new_datasets[1][0],new_datasets[1][1],new_datasets[0][0],new_datasets[0][3])\n",
    "            #es = self.add_r(es,new_datasets[1][0],new_datasets[1][1],new_datasets[0][0],new_datasets[0][3])\n",
    "\n",
    "        #es = add_r('ip_clms','ip_clm','claim_data','claims_frn')\n",
    "        print('es is',es)\n",
    "\n",
    "\n",
    "\n",
    "        ##################################\n",
    "          # get interesting values #\n",
    "        ##################################\n",
    "\n",
    "        if self.pivotings:\n",
    "            for pivoting in self.pivotings:\n",
    "                alias = self.get_info(pivoting['entity'],'alias')\n",
    "                for i,col in enumerate(pivoting['columns']):\n",
    "                    es[alias][col].interesting_values = pivoting['values'][i]\n",
    "                    print('interesting finished')\n",
    "                    #print('check cutoff in get_entityset(): ',cutoff.info())\n",
    "\n",
    "        print('get entityset done')\n",
    "        return es,cut_off_df\n",
    "\n",
    "\n",
    "        # get dfs result\n",
    "    def run_dfs(self,es=None,cutoff = None,saved_feature = None, featureonly = False, mat_name = 'all_fm', fet_name ='Whole_schema'):\n",
    "        # save_res decides whether to save dfs resutls to config\n",
    "        # mat_name is the key name of the saved matrix result from dfs; fet_name is the kay name of the saved features\n",
    "        #start = time.time()\n",
    "        #print('time window values',time_window)\n",
    "        if not es:\n",
    "            es,cut_off = self.get_entityset()\n",
    "        #ignore_vars = self.get_ignore_vars()\n",
    "        agg_used,tfm_used,whr_pmts = self.get_prmts()\n",
    "        #print('get primitives over')\n",
    "        #print('trans is: ',tfm_used)\n",
    "        #start2 = time.time()\n",
    "        dfs_res =  ft.dfs(target_entity=self.target,\n",
    "                                entityset=es,\n",
    "                                #cutoff_time=self.config['cutoff'],\n",
    "                                agg_primitives=agg_used,\n",
    "                                #agg_primitives=[],\n",
    "                                trans_primitives=tfm_used,\n",
    "                                #ignore_entities=['matched_clm'],\n",
    "                                where_primitives=whr_pmts,\n",
    "                                ignore_variables=self.ig_vars,\n",
    "                                cutoff_time = cutoff,\n",
    "                                features_only = featureonly\n",
    "                               )\n",
    "        #end2 = time.time()\n",
    "        #print('get dfs over with time: ',end2-start2)\n",
    "        '''\n",
    "        if not featureonly:\n",
    "            self.config[fet_name] = dfs_res[1]\n",
    "            self.config[mat_name] = dfs_res[0]\n",
    "            self.config['index_save']=dfs_res[0].index\n",
    "\n",
    "        else:\n",
    "            self.config[fet_name] = dfs_res\n",
    "            print('featureonly')\n",
    "        '''\n",
    "        return dfs_res\n",
    "\n",
    "    def calculate_matrix(self,es,features,cutoff):\n",
    "        mtrx = ft.calculate_feature_matrix(features=features, entityset=es,cutoff_time=cutoff)\n",
    "        return mtrx\n",
    "\n",
    "    def recover_col(self,df,cols,key):\n",
    "        df = df.reset_index()\n",
    "        df[cols] = df[key].str.split(\"_\",n=len(cols)-1,expand=True)\n",
    "        df.drop(columns=key,inplace=True)\n",
    "        return df\n",
    "\n",
    "    def deal_colName(self,df):\n",
    "        rep_names=[]\n",
    "        for i in df.columns:\n",
    "            rep_names.append(i.replace('(','<').replace(')','>').replace('=','@').replace('.','/').replace(',','/').replace(' ',''))\n",
    "        df.columns=rep_names\n",
    "        return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. process after schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_cols(features,keyword=None,feature_lst=None,status='check'):\n",
    "    # keyword should be a list of segments (separate segemetns) of the feature names\n",
    "    # for example ['SP','_N'] in feature name: IF(claimant.idx WHERE age_SP = <= 18_N)\n",
    "    if keyword:\n",
    "        pattern = '.*'+'.*'.join(keyword)+'.*'\n",
    "        pattern =  re.compile(pattern)\n",
    "        selected = [f.get_name() for f in features if pattern.match(f.get_name())]\n",
    "    if feature_lst:\n",
    "        selected = feature_lst\n",
    "     \n",
    "    return selected if status=='check' else [f for f in features if not f.get_name() in selected]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flk",
   "language": "python",
   "name": "flk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
